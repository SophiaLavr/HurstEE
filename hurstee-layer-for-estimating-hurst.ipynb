{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11345187,"sourceType":"datasetVersion","datasetId":7098592}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q mfdfa\n!pip install -q andi-datasets \n!pip install -q stochastic\n!pip install -q tensorflow==2.8.0 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:14:03.257801Z","iopub.execute_input":"2025-06-01T17:14:03.258050Z","iopub.status.idle":"2025-06-01T17:15:41.538733Z","shell.execute_reply.started":"2025-06-01T17:14:03.258027Z","shell.execute_reply":"2025-06-01T17:15:41.536687Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.8.0 which is incompatible.\ntensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.8.0 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# HurstEE: Differentiable Neural Network Layer for Estimating Hurst and Anomalous Diffusion Exponents ","metadata":{}},{"cell_type":"markdown","source":"We introduce an implementation of the Time-Averaged Mean Squared Displacement (TA-MSD) method, equivalent to the 2nd-order Generalized Hurst Exponent (GHE) method, as a non-trainable, differentiable layer in TensorFlow and PyTorch frameworks. Our proposed layer effectively addresses the core challenges identified in the 2nd AnDi Challenge, including trajectory heterogeneity and short trajectory segments. The layer also seamlessly handles missing (NaN) values, facilitating integration into complex neural network architectures designed for analyzing heterogeneous trajectories. The proposed differentiable Hurst exponent estimation layer offers simplicity in deployment, eliminating the need for training and reducing computational overhead.","metadata":{}},{"cell_type":"markdown","source":"## Comparison of HurstEE vs various Hurst exponent estimation methods on AnDi-2020 and FBM Datasets","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/randimodel/randi-main')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:41.541076Z","iopub.execute_input":"2025-06-01T17:15:41.541603Z","iopub.status.idle":"2025-06-01T17:15:41.547942Z","shell.execute_reply.started":"2025-06-01T17:15:41.541550Z","shell.execute_reply":"2025-06-01T17:15:41.546588Z"},"_kg_hide-input":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:41.549434Z","iopub.execute_input":"2025-06-01T17:15:41.549906Z","iopub.status.idle":"2025-06-01T17:15:41.607990Z","shell.execute_reply.started":"2025-06-01T17:15:41.549864Z","shell.execute_reply":"2025-06-01T17:15:41.606769Z"},"_kg_hide-input":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# HurstEE with TensorFlow","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\nclass HurstEE(tf.keras.layers.Layer):\n    def __init__(self, use_correction=False, **kwargs):\n        \"\"\"\n        Initialization of the HurstEE layer.\n\n        Args:\n            use_correction (bool): Whether to apply TEA-MSD&variance correction. Default is False.\n\n        \"\"\"\n        super(HurstEE, self).__init__(**kwargs)\n        self.use_correction = use_correction\n\n\n    @staticmethod\n    def nanmean(tensor, axis=None, keepdims=False):\n        \"\"\"\n        Analog of np.nanmean, ignoring NaN values.\n\n        Args:\n            tensor (tf.Tensor): Input tensor.\n            axis (int or tuple, optional): Axis or axes along which to compute the mean.\n            keepdims (bool, optional): If True, retains reduced dimensions.\n\n        Returns:\n            tf.Tensor: Mean value with NaNs ignored.\n        \"\"\"\n        mask = ~tf.math.is_nan(tensor)\n        masked_tensor = tf.where(mask, tensor, tf.zeros_like(tensor))\n        count = tf.reduce_sum(tf.cast(mask, tf.float32), axis=axis, keepdims=keepdims)\n        sum_ = tf.reduce_sum(masked_tensor, axis=axis, keepdims=keepdims)\n        return sum_ / (count + 1e-8)\n\n    @staticmethod\n    def nanstd(tensor, axis=None, keepdims=False):\n        \"\"\"\n        Analog of np.nanstd, ignoring NaN values.\n\n        Args:\n            tensor (tf.Tensor): Input tensor.\n            axis (int or tuple, optional): Axis or axes along which to compute the standard deviation.\n            keepdims (bool, optional): If True, retains reduced dimensions.\n\n        Returns:\n            tf.Tensor: Standard deviation with NaNs ignored.\n        \"\"\"\n        mean_ = HurstEE.nanmean(tensor, axis=axis, keepdims=True)\n        sq_diff = tf.square(tensor - mean_)\n        var_ = HurstEE.nanmean(sq_diff, axis=axis, keepdims=keepdims)\n        return tf.sqrt(var_ + 1e-8)\n\n\n    def tf_normalize_distribution(self, arr, target_mean, T):\n        \"\"\"\n        Normalize the distribution with correction.\n\n        Args:\n            arr (tf.Tensor): Input array.\n            target_mean (tf.Tensor): Target mean value.\n            T (tf.Tensor): Effective time/count indicator.\n\n        Returns:\n            tf.Tensor: Normalized array.\n        \"\"\"\n        current_mean = self.nanmean(arr)\n        current_std = self.nanstd(arr)\n        adjustment_term = tf.maximum(current_std**2 - 0.92 / T, 0)**0.5\n        standardized = (arr - current_mean) / (current_std + 1e-8)\n        return standardized * adjustment_term + target_mean\n\n    def call(self, inputs):\n        \"\"\"\n        Forward pass of the layer.\n\n        Args:\n            inputs (tf.Tensor): Input tensor of shape (batch_size, timesteps, input_channels).\n\n        Returns:\n            tf.Tensor: Estimated alpha values.\n        \"\"\"\n        batch_size = tf.shape(inputs)[0]\n        timesteps = tf.shape(inputs)[1]\n        input_channels = tf.shape(inputs)[2]\n\n        # Count non-NaN elements across the batch\n        not_nan_mask = ~tf.math.is_nan(inputs)\n        not_nan_count = tf.reduce_sum(tf.cast(not_nan_mask, tf.float32))\n\n        epsilon = 1e-14\n        x_all = inputs\n        max_lag = 4\n        msds = []\n        teamsd_list = []\n\n        # --- Compute MSD and TEA-MSD ---\n        for lag in range(1, max_lag + 1):\n            displacements = x_all[:, lag:, :] - x_all[:, :-lag, :]\n            squared_displacements = tf.square(displacements)\n            squared_displacements_sum = tf.reduce_sum(squared_displacements, axis=2)\n\n            # For MSD – trim each row\n            msd = self.nanmean(\n                squared_displacements_sum,\n                axis=1\n            )\n            msds.append(msd)\n\n            # For TEA-MSD – trim the array as a whole\n            teamsd_lag = self.nanmean(\n                squared_displacements_sum\n            )\n            teamsd_list.append(teamsd_lag)\n\n        msds = tf.stack(msds, axis=1)             # (batch_size, max_lag)\n        teamsd = tf.stack(teamsd_list, axis=0)    # (max_lag,)\n\n        t_lags = tf.range(1, max_lag + 1, dtype=tf.float32)\n        log_t_lags = tf.math.log(t_lags + epsilon)\n\n        # --- Compute TEA-MSD alpha ---\n        log_teamsd = tf.math.log(teamsd + epsilon)\n        mean_x = self.nanmean(log_t_lags)\n        mean_x2 = self.nanmean(tf.square(log_t_lags))\n        mean_y_teamsd = self.nanmean(log_teamsd)\n        mean_xy_teamsd = self.nanmean(log_t_lags * log_teamsd)\n\n        numerator_teamsd = mean_xy_teamsd - mean_x * mean_y_teamsd\n        denominator = mean_x2 - mean_x**2\n        alpha_teamsd = numerator_teamsd / denominator\n\n        # --- Compute alpha for the batch ---\n        log_msds = tf.math.log(msds + epsilon)   # (batch_size, max_lag)\n        mean_y = self.nanmean(log_msds, axis=1)  # (batch_size,)\n        mean_xy = self.nanmean(log_t_lags * log_msds, axis=1)\n        numerator = mean_xy - mean_x * mean_y\n        alpha = numerator / denominator          # (batch_size,)\n\n        # --- Replace alpha with NaN if the last MSD < 1e-6 ---\n        last_msd = msds[:, -1]  # (batch_size,)\n        mask_nan = last_msd < 1e-6\n        alpha = tf.where(mask_nan, tf.fill(tf.shape(alpha), np.nan), alpha)\n\n        # --- Compute T based on the number of valid trajectories ---\n        valid_mask = ~mask_nan\n        valid_count = tf.reduce_sum(tf.cast(valid_mask, tf.float32))\n        T = not_nan_count / (valid_count * tf.cast(input_channels, tf.float32))\n\n        # --- Apply normalization correction if enabled ---\n        if self.use_correction:\n            alpha = self.tf_normalize_distribution(alpha, alpha_teamsd, T)\n\n        # --- Clip alpha to the valid range ---\n        H = tf.clip_by_value(alpha, clip_value_min=0.0001, clip_value_max=1.999) / 2\n\n        return H","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:41.610918Z","iopub.execute_input":"2025-06-01T17:15:41.611242Z","iopub.status.idle":"2025-06-01T17:15:50.467458Z","shell.execute_reply.started":"2025-06-01T17:15:41.611214Z","shell.execute_reply":"2025-06-01T17:15:50.466010Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# HurstEE with PyTorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass PyHurstEE(nn.Module):\n    def __init__(self, use_correction=False):\n        super(PyHurstEE, self).__init__()\n        self.use_correction = use_correction\n\n    @staticmethod\n    def nanmean(tensor, dim=None, keepdim=False, eps=1e-8):\n        mask = ~torch.isnan(tensor)\n        masked = torch.where(mask, tensor, torch.zeros_like(tensor))\n        count = mask.sum(dim=dim, keepdim=keepdim).to(tensor.dtype)\n        sum_ = masked.sum(dim=dim, keepdim=keepdim)\n        return sum_ / (count + eps)\n\n    @staticmethod\n    def nanstd(tensor, dim=None, keepdim=False, eps=1e-8):\n        mean_ = PyHurstEE.nanmean(tensor, dim=dim, keepdim=True, eps=eps)\n        sq_diff = (tensor - mean_)**2\n        var = PyHurstEE.nanmean(sq_diff, dim=dim, keepdim=keepdim, eps=eps)\n        return torch.sqrt(var + eps)\n\n\n    def normalize_distribution(self, arr, target_mean, T, eps=1e-8):\n        current_mean = self.nanmean(arr)\n        current_std = self.nanstd(arr)\n        adjustment = torch.sqrt(torch.clamp(current_std**2 - 0.92 / T, min=0.0))\n        standardized = (arr - current_mean) / (current_std + eps)\n        return standardized * adjustment + target_mean\n\n    def forward(self, inputs):\n        if isinstance(inputs, np.ndarray):\n            inputs = torch.from_numpy(inputs).float()\n        batch, timesteps, channels = inputs.shape\n        not_nan = ~torch.isnan(inputs)\n        not_nan_count = not_nan.sum().to(inputs.dtype)\n\n        max_lag = 4\n        msds = []\n        teamsd_list = []\n\n        for lag in range(1, max_lag + 1):\n            disp = inputs[:, lag:, :] - inputs[:, :-lag, :]\n            sq = disp**2\n            sq_sum = sq.sum(dim=2)\n\n            msd = PyHurstEE.nanmean(sq_sum, dim=1)\n            msds.append(msd)\n\n\n            teamsd_lag = PyHurstEE.nanmean(sq_sum)\n            teamsd_list.append(teamsd_lag)\n\n        msds = torch.stack(msds, dim=1)\n        teamsd = torch.stack(teamsd_list)\n\n        eps = 1e-14\n        t_lags = torch.arange(1, max_lag+1, dtype=inputs.dtype, device=inputs.device)\n        log_t = torch.log(t_lags + eps)\n\n        log_teamsd = torch.log(teamsd + eps)\n        mean_x = PyHurstEE.nanmean(log_t)\n        mean_x2 = PyHurstEE.nanmean(log_t**2)\n        mean_y_tea = PyHurstEE.nanmean(log_teamsd)\n        mean_xy_tea = PyHurstEE.nanmean(log_t * log_teamsd)\n        num_tea = mean_xy_tea - mean_x * mean_y_tea\n        den = mean_x2 - mean_x**2\n        alpha_tea = num_tea / den\n\n        log_msds = torch.log(msds + eps)\n        mean_y = PyHurstEE.nanmean(log_msds, dim=1)\n        mean_xy = PyHurstEE.nanmean(log_t * log_msds, dim=1)\n        num = mean_xy - mean_x * mean_y\n        alpha = num / den\n\n        last = msds[:, -1]\n        mask = last < 1e-6\n        alpha = alpha.masked_fill(mask, float('nan'))\n\n        valid = ~mask\n        valid_count = valid.sum().to(inputs.dtype)\n        T = not_nan_count / (valid_count * channels)\n\n        if self.use_correction:\n            alpha = self.normalize_distribution(alpha, alpha_tea, T)\n\n        H = torch.clamp(alpha, min=0.0001, max=1.999) / 2\n        return H","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:50.472945Z","iopub.execute_input":"2025-06-01T17:15:50.473323Z","iopub.status.idle":"2025-06-01T17:15:55.859262Z","shell.execute_reply.started":"2025-06-01T17:15:50.473290Z","shell.execute_reply":"2025-06-01T17:15:55.858086Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import andi_datasets\nfrom andi_datasets.datasets_challenge import challenge_theory_dataset\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.models import load_model\nfrom utils import data_norm, data_reshape, many_net_uhd, my_atan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:55.860635Z","iopub.execute_input":"2025-06-01T17:15:55.861100Z","iopub.status.idle":"2025-06-01T17:15:57.252294Z","shell.execute_reply.started":"2025-06-01T17:15:55.861062Z","shell.execute_reply":"2025-06-01T17:15:57.251092Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## RANDI weights","metadata":{}},{"cell_type":"code","source":"randi25 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_25.h5')\nrandi65 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_65.h5')  \nrandi125 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_125.h5')\nrandi225 = load_model('/kaggle/input/randimodel/randi-main/nets/inference_nets/1d/inference_1D_225.h5')    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:57.253482Z","iopub.execute_input":"2025-06-01T17:15:57.254154Z","iopub.status.idle":"2025-06-01T17:15:59.221954Z","shell.execute_reply.started":"2025-06-01T17:15:57.254113Z","shell.execute_reply":"2025-06-01T17:15:59.220723Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pywt\nfrom sklearn.linear_model import LinearRegression\n\ndef calculate_Hw(X, wavelet=\"db4\"):\n    \"\"\"\n    Xcum -  time series\n    wavelet - name of wavelet\n    level - decomposition level\n    \"\"\"\n    \n    level = int(np.floor(np.log2(len(X))) - 1)\n    # Wavelet transformation\n    coeffs = pywt.wavedec(X, wavelet, level=level, mode=\"periodization\") # mode='symmetric'\n    \n    # Wavelet energy\n    E = np.array(list(reversed([np.sum(i ** 2)/len(i) for i in coeffs[1:]])))\n  \n    # Calculation of Hurst exponent\n    reg = LinearRegression().fit(np.arange(len(coeffs) - 1).reshape(-1, 1), np.log2(E))\n    H = np.abs((reg.coef_ + 1) / 2)\n    \n    return H","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.223087Z","iopub.execute_input":"2025-06-01T17:15:59.223380Z","iopub.status.idle":"2025-06-01T17:15:59.469263Z","shell.execute_reply.started":"2025-06-01T17:15:59.223357Z","shell.execute_reply":"2025-06-01T17:15:59.468034Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from MFDFA import MFDFA\ndef mf_dfa(y):\n\n    lag = np.arange(1,24)\n\n    q_list = [1,2]  \n    \n    lag, Fq = MFDFA(y, lag=lag, q=q_list, order=1)  # Fq.shape = (len(lag), len(q))\n    \n    log_lag = np.log(lag)\n    h_q = np.array([\n        np.polyfit(log_lag, np.log(Fq[:, i]), 1)[0]\n        for i in range(len(q_list))\n    ])\n\n  \n    return np.clip(h_q[1], 0.001, 0.999)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.470552Z","iopub.execute_input":"2025-06-01T17:15:59.471724Z","iopub.status.idle":"2025-06-01T17:15:59.502949Z","shell.execute_reply.started":"2025-06-01T17:15:59.471685Z","shell.execute_reply":"2025-06-01T17:15:59.501711Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ta_msd():\n    def __init__(self):\n        ''' Contains mean squared displacement (MSD) based methods to analyze trajectories.  '''\n        \n    def trim_array1s(self, arr, percent=0):\n        \"\"\"\n        Removes the specified percentage of the largest values from each row of the array, ignoring NaN.\n        \n        Parameters:\n        arr (numpy.ndarray): Input data array.\n        percent (float): Percentage of the largest values to remove from each row (0 <= percent < 100).\n        \n        Returns:\n        numpy.ndarray: Array with the largest values removed from each row.\n        \"\"\"\n        if not (0 <= percent < 100):\n            raise ValueError(\"The 'percent' parameter must be in the range [0, 100).\")\n        \n        if not isinstance(arr, np.ndarray):\n            raise TypeError(\"Input data must be a numpy array.\")\n        \n        if len(arr) == 0:\n            return np.array([])  # Return an empty array if input array is empty\n        \n        result = np.empty_like(arr, dtype=float)\n        \n        for i, row in enumerate(arr):\n            # Select only non-NaN values from the row\n            non_nan_values = row[~np.isnan(row)]\n            \n            if len(non_nan_values) == 0:\n                # If all values in the row are NaN, leave it unchanged\n                result[i] = row\n                continue\n            \n            # Sort non-NaN values in ascending order\n            sorted_values = np.sort(non_nan_values)\n            \n            # Calculate the number of elements to remove\n            n_non_nan = len(sorted_values)\n            k = int(np.ceil(percent / 100 * n_non_nan))  # Number of elements to remove\n            \n            if k == 0:\n                # If there are no elements to remove, copy the row unchanged\n                result[i] = row\n                continue\n            \n            # Determine the threshold value for removal\n            threshold = sorted_values[-k]\n            \n            # Create a mask to filter values in the current row\n            mask = row <= threshold\n            result[i] = np.where(mask, row, np.nan)  # Replace values above the threshold with NaN\n        \n        return result\n    \n    def trim_array1s_tea(self, arr, percent=0):\n        \"\"\"\n        Removes the specified percentage of the largest values from the entire array, ignoring NaN.\n        The number of elements to remove is calculated based on non-NaN values.\n        The order of values in each row is preserved.\n    \n        Parameters:\n        arr (numpy.ndarray): Input data array.\n        percent (float): Percentage of the largest values to remove (0 <= percent < 100).\n    \n        Returns:\n        numpy.ndarray: Array with the largest values removed.\n        \"\"\"\n        if not (0 <= percent < 100):\n            raise ValueError(\"The 'percent' parameter must be in the range [0, 100).\")\n        \n        if not isinstance(arr, np.ndarray):\n            raise TypeError(\"Input data must be a numpy array.\")\n        \n        if len(arr) == 0:\n            return np.array([])  # Return an empty array if input array is empty\n        \n        # Flatten the array to one dimension, ignoring NaN\n        flattened = arr[~np.isnan(arr)]\n        \n        if len(flattened) == 0:\n            return arr  # If all values are NaN, return the original array\n        \n        # Sort the array without NaN\n        sorted_values = np.sort(flattened)\n        \n        # Calculate the number of elements to remove based on non-NaN values\n        n_non_nan = len(sorted_values)\n        k = int(np.ceil(percent / 100 * n_non_nan))  # Number of elements to remove\n        \n        if k == 0:\n            return arr  # If there are no elements to remove, return the original array\n        \n        # Determine the threshold value for removal\n        threshold = sorted_values[-k] if k > 0 else -np.inf\n        \n        # Create a mask to filter values\n        mask = arr <= threshold\n        result = np.where(mask, arr, np.nan)  # Replace values above the threshold with NaN\n        \n        return result    \n\n\n    def tamsd(self, \n              trajs:np.ndarray, \n              t_lags:np.ndarray,\n              method='MSD'\n             ):\n        '''\n        Calculates the time average mean squared displacement (TA-MSD) of a trajectory at various time lags,\n        \n        Parameters\n        ----------\n        trajs : np.array\n            Set of trajectories of dimenions NxDxT (N: number of trajectories, D: dimension, T: lenght)\n        \n        t_lags : list | np.array\n            Time lags used for the TA-MSD\n        \n        dim : int\n            Dimension of the trajectories (currently only 1 and 2 supported)\n        \n        Returns       \n        -------\n        np.array\n            TA-MSD of each trayectory / t_lag\n            \n        '''\n        tamsd = np.zeros((trajs.shape[0],len(t_lags)), dtype= float)\n        for idx, tlag in enumerate(t_lags):\n            res1 = np.nanmean(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), axis=-1)\n            res4 = np.nanmedian(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), axis=-1)\n            res5 = np.nanquantile(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), 0.632, axis=-1)\n         \n            res3 = np.nanmean(self.trim_array1s(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), 15), axis=-1) * 1.664\n\n            if method=='MSD':\n                    tamsd[:,idx] = res1\n            elif method=='MSDt':\n                    tamsd[:,idx] = res3\n            elif method=='MedianSD':\n                    tamsd[:,idx] = res4            \n            elif method=='QSD':\n                    tamsd[:,idx] = res5     \n\n        return tamsd\n        \n    def teamsd(self, \n              trajs:np.ndarray, \n              t_lags:np.ndarray,\n              method='MSD'\n             ):\n        '''\n        Calculates the time average mean squared displacement (TA-MSD) of a trajectory at various time lags,\n        \n        Parameters\n        ----------\n        trajs : np.array\n            Set of trajectories of dimenions NxDxT (N: number of trajectories, D: dimension, T: lenght)\n        \n        t_lags : list | np.array\n            Time lags used for the TA-MSD\n        \n        dim : int\n            Dimension of the trajectories (currently only 1 and 2 supported)\n        \n        Returns       \n        -------\n        np.array\n            TA-MSD of each trayectory / t_lag\n            \n        '''\n        teamsd = np.zeros((1,len(t_lags)), dtype= float)\n        for idx, tlag in enumerate(t_lags):\n            res1 = np.nanmean(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1))\n            res4 = np.nanmedian(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1))\n            res5 = np.nanquantile(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), 0.632)\n         \n            res3 = np.nanmean(self.trim_array1s_tea(((trajs[:, tlag:, :]-trajs[:, :-tlag, :])**2).sum(-1), 15)) * 1.664\n\n            if method=='MSD':\n                    teamsd[:,idx] = res1\n            elif method=='MSDt':\n                    teamsd[:,idx] = res3\n            elif method=='MedianSD':\n                    teamsd[:,idx] = res4            \n            elif method=='QSD':\n                    teamsd[:,idx] = res5 \n        return teamsd    \n\n    def get_diff_coeff(self, \n                       trajs:np.ndarray, \n                       t_lags:list = None):\n        '''\n        Calculates the diffusion coefficient of a trajectory by means of the linear\n        fitting of the TA-MSD.\n        \n        Parameters\n        ----------\n        traj : np.array\n            Set of trajectories of dimenions NxTxD (N: number of trajectories, T: lenght, D: dimension)\n        \n        t_lags : bool | list\n            Time lags used for the TA-MSD.\n        \n        Returns       \n        -------\n        np.array\n            Diffusion coefficient of the given trajectory.          \n        \n        '''\n        \n        if not t_lags:\n            t_lags = np.arange(1, 5)\n\n        tasmd = self.tamsd(trajs, t_lags) \n\n        res = np.zeros(trajs.shape[0], dtype= float) \n        for idx, row in enumerate(tasmd):\n            if not np.any(np.isnan(row)):\n                res[idx] = np.polyfit(t_lags, row, deg = 1)[0] / 4\n                # print(idx, np.log(row))\n            else:\n                res[idx] = np.NaN\n                # print('-')\n        return res\n    \n    def get_tea_diff_coeff(self, \n                       trajs:np.ndarray, \n                       t_lags:list = None):\n        '''\n        Calculates the diffusion coefficient of a trajectory by means of the linear\n        fitting of the TA-MSD.\n        \n        Parameters\n        ----------\n        traj : np.array\n            Set of trajectories of dimenions NxTxD (N: number of trajectories, T: lenght, D: dimension)\n        \n        t_lags : bool | list\n            Time lags used for the TA-MSD.\n        \n        Returns       \n        -------\n        np.array\n            Diffusion coefficient of the given trajectory.          \n        \n        '''\n        \n        if not t_lags:\n            t_lags = np.arange(1, 5)\n\n        teasmd = self.teamsd(trajs, t_lags) \n\n\n\n        if not np.any(np.isnan(teasmd)):\n            res = np.polyfit(t_lags, teasmd[0], deg = 1)[0] / 4\n            # print(idx, np.log(row))\n        else:\n            res = np.NaN\n            # print('-')\n        return res\n        \n    def get_exponent(self, \n                     trajs:np.ndarray, \n                     t_lags:list = None, method='MSD'):\n        '''\n        Calculates the diffusion coefficient of a trajectory by means of the linear\n        fitting of the TA-MSD.\n        \n        Parameters\n        ----------\n        traj : np.array\n            Set of trajectories of dimenions NxTxD (N: number of trajectories, T: lenght)\n        \n        t_lags : bool | list\n            Time lags used for the TA-MSD.\n        \n        Returns       \n        -------\n        np.array\n            Diffusion coefficient of the given trajectory.          \n        \n        '''\n        \n        # To account for previous versions of this function, we correct if given a single 1D trajectory\n            \n            \n        if not t_lags:\n            t_lags = np.arange(1, 5)\n\n        tasmd = self.tamsd(trajs, t_lags, method)\n\n        res = np.zeros(trajs.shape[0], dtype= float) \n        for idx, row in enumerate(tasmd):\n            if not np.any(np.isnan(row)):\n                res[idx] = np.clip(np.polyfit(np.log(t_lags), np.log(row), deg = 1)[0], 0.001, 1.999)\n                # print(idx, np.log(row))\n            else:\n                res[idx] = np.NaN\n                # print('-')\n        return res       \n\n    def get_tea_exponent(self, \n                     trajs:np.ndarray, \n                     t_lags:list = None, method='MSD'):\n        '''\n        Calculates the diffusion coefficient of a trajectory by means of the linear\n        fitting of the TA-MSD.\n        \n        Parameters\n        ----------\n        traj : np.array\n            Set of trajectories of dimenions NxTxD (N: number of trajectories, T: lenght)\n        \n        t_lags : bool | list\n            Time lags used for the TA-MSD.\n        \n        Returns       \n        -------\n        np.array\n            Diffusion coefficient of the given trajectory.          \n        \n        '''\n        \n        # To account for previous versions of this function, we correct if given a single 1D trajectory\n            \n            \n        if not t_lags:\n            t_lags = np.arange(1, 5)\n\n        teasmd = self.teamsd(trajs, t_lags, method)\n\n\n\n        if not np.any(np.isnan(teasmd)):\n            res = np.clip(np.polyfit(np.log(t_lags), np.log(teasmd[0]), deg = 1)[0], 0.001, 1.999)\n            # print(idx, np.log(row))\n        else:\n            res = np.NaN\n            # print('-')\n        return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.504222Z","iopub.execute_input":"2025-06-01T17:15:59.504542Z","iopub.status.idle":"2025-06-01T17:15:59.535423Z","shell.execute_reply.started":"2025-06-01T17:15:59.504514Z","shell.execute_reply":"2025-06-01T17:15:59.534272Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\"\"\"\nCreated on Wes June 21 16:44:17 2023\n\n@author: zqfeng\n\"\"\"\n\nimport sys\nimport numpy as np\nimport scipy.optimize as op\n\n\nclass AddMethods(object):\n\n    def __init__(self):\n        None\n\n    def Divisors(self, N: int, minimal=20) -> list:\n        D = []\n        for i in range(minimal, N // minimal + 1):\n            if N % i == 0:\n                D.append(i)\n        return D\n\n    def findOptN(self, N: int, minimal=20) -> int:\n        \"\"\"\n        Find such a natural number OptN that possesses the largest number of\n        divisors among all natural numbers in the interval [0.99*N, N]\n        \"\"\"\n        N0 = int(0.99 * N)\n        # The best length is the one which have more divisiors\n        Dcount = [len(self.Divisors(i, minimal)) for i in range(N0, N + 1)]\n        OptN = N0 + Dcount.index(max(Dcount))\n        return OptN\n\n    def OLE_linprog(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:\n        '''\n        Ax = b (Given A & b, try to derive x)\n\n        Parameters\n        ----------\n        A : matrix like. With shape m x n.\n        b : array like. With shape n x 1.\n\n        Returns\n        -------\n        x : Minimal L1 norm solution of the system of equations.\n\n        Reference\n        ---------\n        YAO Jian-kang. An Algorithm for Minimizing l1-Norm to Overdetermined\n        Linear Eguations[J]. JIANGXI SCE7NICE, 2007, 25(1): 1-4.\n        (Available at:\n        http://d.g.wanfangdata.com.cn/Periodical_jxkx200701002.aspx)\n\n        Version: 1.0 writen by z.q.feng @2022.03.13\n        '''\n        A, b = np.array(A), np.array(b)\n        if np.size(A, 0) < np.size(A, 1):\n            raise ValueError('Matrix A rows must greater than columns!')\n        m, n = A.shape\n        # Trans A into two matrix(n x n and (m - n) x n)\n        A1, A2 = A[:n, :], A[n:, :]\n        if np.linalg.matrix_rank(A) >= n:\n            # inverse of A1\n            A1_ = np.linalg.pinv(A1)\n        else:\n            # Generalized inverse of A1\n            A1_ = np.linalg.pinv(A1)\n        # c_ij = A2 * A1_\n        c = np.dot(A2, A1_)\n        # r(n+1:m) = A2*inv(A1)*r(1:n) + d\n        d = np.dot(c, b[:n]) - b[n:]\n        # Basic-Pursuit, target function = sum(u, v)\n        t = np.ones([2 * m, 1])\n        # Aeq_ = [c I(m-n)]\n        Aeq_ = np.hstack([-c, np.eye(m - n, m - n)])\n        # Aeq[u v] = Aeq_ * (u - v)\n        Aeq = np.hstack([Aeq_, -Aeq_])\n        # u, v > 0\n        bounds = [(0, None) for i in range(2 * m)]\n        # r0 = [u; v]\n        r0 = op.linprog(t, A_eq=Aeq, b_eq=d, bounds=bounds,\n                        method='revised simplex')['x']\n        # Minimal L1-norm residual vector, r = u - v\n        r = np.array([r0[:m] - r0[m:]])\n        # Solving compatible linear equations Ax = b + r\n        # Generalized inverse solution\n        x = np.linalg.pinv(A).dot(b + r.T)\n        return x\n\n    def FixedPointSolver(self, fun, x0, eps=1e-6, **kwargs):\n        \"\"\"\n        Solving the fixed points.\n        \"\"\"\n        k, k_max = 0, 10000\n        x_guess, dist = x0, 1\n        while dist > eps and k < k_max:\n            x_improved = fun(x_guess, **kwargs)\n            dist = abs(x_improved - x_guess)\n            x_guess = x_improved\n            k += 1\n        return x_guess\n\n    def LocalMin(self, fun, interval: list, **kwargs):\n        \"\"\"\n            The method used is a combination of  golden  section  search  and\n        successive parabolic interpolation.  convergence is never much slower\n        than  that  for  a  Fibonacci search.  If fun has a continuous second\n        derivative which is positive at the minimum (which is not  at  ax  or\n        bx),  then  convergence  is  superlinear, and usually of the order of\n        about  1.324....\n            The function fun is never evaluated at two points closer together\n        than  eps*abs(fmin)+(tol/3), where eps is  approximately  the  square\n        root  of  the  relative  machine  precision.   if  fun  is a unimodal\n        function and the computed values of  fun  are  always  unimodal  when\n        separated  by  at least  eps*abs(x)+(tol/3), then  fmin  approximates\n        the abcissa of the global minimum of fun on the interval  ax,bx  with\n        an error less than  3*eps*abs(fmin)+tol.  if  fun  is  not  unimodal,\n        then fmin may approximate a local, but perhaps non-global, minimum to\n        the same accuracy.\n            This function subprogram is a slightly modified  version  of  the\n        python3 procedure  localmin  given in Ref[1] page79.\n\n        Parameters\n        ----------\n        fun      : Abcissa approximating the point where fun attains a minimum.\n        interval : Iterative interval of target minimum point.\n\n        Returns\n        -------\n        The Hurst exponent of time series ts.\n\n        References\n        ----------\n        [1] Richard Brent, Algorithms for Minimization without Derivatives,\n            Prentice-Hall, Inc. (1973).\n\n        Written by z.q.feng (2023.06.07).\n        \"\"\"\n        a, b = min(interval), max(interval)\n        # c is the squared inverse of the golden ratio\n        c, d, e = (3 - 5**0.5) / 2, 0, 0\n        # eps is approximately the square root of relative machine precision.\n        tol = sys.float_info.epsilon**0.25\n        eps = tol**2\n        # the smallest 1.000... > 1 : tol1 = 1 + eps**2\n        v = w = x = a + c * (b - a)\n        fv = fw = fx = fun(x, **kwargs)\n        # main loop starts here\n        while True:\n            m = (a + b) / 2\n            tol1 = eps * abs(x) + tol / 3\n            tol2 = tol1 * 2\n            # check stopping criterion\n            if abs(x - m) <= tol2 - (b - a) / 2:\n                break\n            p = q = r = 0\n            # fit parabola\n            if abs(e) > tol1:\n                r = (x - w) * (fx - fv)\n                q = (x - v) * (fx - fw)\n                p = (x - v) * q - (x - w) * r\n                q = (q - r) * 2\n                if q > 0:\n                    p *= -1\n                else:\n                    q *= -1\n                r, e = e, d\n            if abs(p) >= abs(0.5 * q * r) or p <= q * (a-x) or p >= q * (b-x):\n                # a golden-section step\n                e = (b if x < m else a) - x\n                d = c * e\n            else:\n                # a parabolic-interpolation step\n                d = p / q\n                u = x + d\n                # f must not be evaluated too close to ax or bx\n                if u - a < tol2 or b - u < tol2:\n                    d = tol1 if x < m else -tol1\n            # f must not be evaluated too close to x\n            if abs(d) >= tol1:\n                u = x + d\n            elif d > 0:\n                u = x + tol1\n            else:\n                u = x - tol1\n            fu = fun(u, **kwargs)\n            # update  a, b, v, w, and x\n            if fu <= fx:\n                if u < x:\n                    b = x\n                else:\n                    a = x\n                v, fv, w, fw, x, fx = w, fw, x, fx, u, fu\n            else:\n                if u < x:\n                    a = u\n                else:\n                    b = u\n                if fu <= fw or w == x:\n                    v, fv, w, fw = w, fw, u, fu\n                elif fu <= fv or v == x or v == w:\n                    v, fv = u, fu\n        # end of main loop\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.536516Z","iopub.execute_input":"2025-06-01T17:15:59.536880Z","iopub.status.idle":"2025-06-01T17:15:59.561978Z","shell.execute_reply.started":"2025-06-01T17:15:59.536846Z","shell.execute_reply":"2025-06-01T17:15:59.560704Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pywt\nimport numpy as np\nfrom stochastic.processes.continuous import FractionalBrownianMotion as fbm\nfrom math import pi, gamma\nfrom scipy import fft, stats\n\n\nclass HurstIndexSolver(AddMethods):\n\n    def __init__(self):\n        None\n\n    def __FitCurve(self, Scale: list, StatisticModel: list,\n                   method='L2') -> float:\n        \"\"\"\n        Fitting scale ~ statisticModel in a log-log plot.\n        \"\"\"\n        Scale = np.log10(np.array([Scale]))\n        Scale = np.vstack([Scale, np.ones(len(StatisticModel))]).T\n        if method == 'L2':\n            # slope = np.polyfit(np.log10(Cm), np.log10(AM), 1)[0]\n            slope, c = np.linalg.lstsq(\n                Scale,\n                np.log10(StatisticModel),\n                rcond=-1\n            )[0]\n        elif method == 'L1':\n            slope, c = super().OLE_linprog(\n                Scale,\n                np.array([np.log10(StatisticModel)]).T\n            )\n            slope = slope[0]\n        # slope = np.polyfit(np.log(Scale), np.log(StatisticModel), deg = 1)[0]\n        return slope\n\n    def EstHurstClustering(self, ts, order: float, minimal=10,\n                           method='L2') -> float:\n        \"\"\"\n        Calculate the Hurst exponent using Clustering Method.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 10.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Absolute Moments Method (AM).\n\n        Reference\n        ---------\n        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n        [J]. 2021.\n\n        written by z.q.feng at 2022.09.05\n        \"\"\"\n        N = len(ts)\n        # make sure m is large and (N / m) is large\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n\n        ts = ts[N - OptN:]\n        # The mean for series\n        Avg = np.mean(ts)\n\n        CM = []\n        for m in M:\n            k = OptN // m\n            # remove the redundant data at the begin\n            # each row is a subseries with N m\n            Xsub = np.reshape(ts, [k, m])\n            # mean of each suseries\n            Xm = np.mean(Xsub, axis=1)\n            # order == 1 : Absolute Moments Method\n            # order == 2 : Aggregated Variance Method\n            CM.append(np.mean(abs(Xm - Avg)**order))\n\n        slope = self.__FitCurve(M, CM, method=method)\n        return slope / order + 1\n\n    def EstHurstAbsoluteMoments(self, ts, minimal=20, method='L2') -> float:\n        \"\"\"\n        Calculate the Hurst exponent using Cluster Method.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 10.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Absolute Moments Method (AM).\n\n        Reference\n        ---------\n        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n        [J]. 2021.\n\n        written by z.q.feng at 2022.09.05\n        \"\"\"\n        N = len(ts)\n        # make sure m is large and (N / m) is large\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n\n        ts = ts[N - OptN:]\n        # The mean for series\n        Avg = np.mean(ts)\n\n        AM = []\n        for m in M:\n            k = OptN // m\n            # remove the redundant data at the begin\n            # each row is a subseries with N m\n            Xsub = np.reshape(ts, [k, m])\n            # mean of each suseries\n            Xm = np.mean(Xsub, axis=1)\n            AM.append(np.linalg.norm(Xm - Avg, 1) / len(Xm))\n\n        slope = self.__FitCurve(M, AM, method=method)\n        return slope + 1\n\n    def EstHurstAggregateVariance(self, ts, minimal=12, method='L2') -> float:\n        \"\"\"\n        Calculate the Hurst exponent using Cluster Method.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 10.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Aggregate Variance Method (AV).\n\n        Reference\n        ---------\n        Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation methods\n        [J]. 2021.\n\n        written by z.q.feng at 2022.09.05\n        \"\"\"\n        N = len(ts)\n        # The mean for series\n        # Avg = np.mean(ts)\n\n        # make sure m is large and (N / m) is large\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n\n        AV = []\n        for m in M:\n            k = OptN // m\n            # remove the redundant data at the begin\n            # each row is a subseries with N m\n            Xsub = np.reshape(ts[N - OptN:], [k, m])\n            # mean of each suseries\n            Xm = np.mean(Xsub, axis=1)\n            AV.append(np.var(Xm, ddof=0))\n            # AV.append(np.var(Xm - Avg, ddof=1))\n\n        slope = self.__FitCurve(M, AV, method=method)\n        return slope / 2 + 1\n\n    def EstHurstDFAnalysis(self, ts, minimal=12, method='L2') -> float:\n        \"\"\"\n        DFA Calculate the Hurst exponent using DFA analysis.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 10.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Detrended Fluctuation Analysis (DFA).\n\n        References\n        ----------\n        [1] C.-K.Peng et al. (1994) Mosaic organization of DNA nucleotides,\n        Physical Review E 49(2), 1685-1689.\n        [2] R.Weron (2002) Estimating long range dependence: finite sample\n        properties and confidence intervals, Physica A 312, 285-299.\n\n        Written by z.q.feng (2022.09.23).\n        Based on dfa.m orginally written by afal Weron (2011.09.30).\n        \"\"\"\n        DF = []\n        N = len(ts) + 1\n        y = np.concatenate([[0], np.cumsum(ts)])\n\n        OptN = super().findOptN(len(ts), minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n\n        for m in M:\n            k = OptN // m\n            Y = np.reshape(y[N - OptN:], [m, k], order='F')\n            F = np.copy(Y)\n            # t = 1, 2, ..., m\n            t = np.linspace(1, m, m)\n            for i in range(k):\n                p = np.polyfit(t, Y[:, i], 1)\n                F[:, i] = Y[:, i] - t * p[0] - p[1]\n            DF.append(np.mean(np.std(F)))\n\n        slope = self.__FitCurve(M, DF, method=method)\n        return slope\n\n    def __getBox(self, j: int) -> int:\n        \"\"\"\n        [2^{(j-1)/4}] for j in (11, 12, 13, ...) if k > 4\n        \"\"\"\n        if j < 5:\n            return j\n        else:\n            return int(2 ** ((j + 5) / 4))\n\n    def EstHurstHiguchi(self, ts, minimal=11, method='L2') -> float:\n        \"\"\"\n        Calculate the Hurst exponent using Higuchi Method.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using Higuchi Method (HM).\n\n        References\n        ----------\n        [1] Higuchi T. Approach to an irregular time series on the basis of\n            the fractal theory[J]. Physica D: Nonlinear Phenomena, 1988, 31(2):\n            277-283.\n        \"\"\"\n        N = len(ts) + 1\n        Lm, Cm = [], []\n        # FGN --diff--> Gaussian\n        Y = np.concatenate([[0], np.cumsum(ts)])\n\n        for j in range(1, minimal):\n            Lk = []\n            m = self.__getBox(j)\n            Cm.append(m)\n            k = N // m\n            Xsub = np.reshape(Y[N % m:], [k, m])\n            for i in range(1, k):\n                Lk.append(abs(Xsub[i] - Xsub[i - 1]))\n            # Lm = np.mean(np.array(Lk), axis=0) * (N - 1) / k / k\n            Lm.append(np.mean(Lk) * (N - 1) / m / m)\n\n        slope = self.__FitCurve(Cm, Lm, method=method)\n        return slope + 2\n\n    def EstHurstRegrResid(self, ts, minimal=20, method='L2') -> float:\n        '''\n        Variance of the regression residuals (VRR) for Hurst Index.\n        '''\n        Sigma = []\n        N = len(ts)\n\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)[:-3]\n\n        for m in M:\n            res = []\n            k = OptN // m\n            t = np.linspace(1, m, m)\n            X = np.reshape(ts[N - OptN:], [k, m])\n            for i in range(k):\n                Y = np.cumsum(X[i] - np.mean(X[i]))\n                a, b = np.polyfit(t, Y, 1)\n                res.append(np.std(Y - a * t - b, ddof=1))\n            Sigma.append(np.mean(res))\n\n        slope = self.__FitCurve(M, Sigma, method=method)\n        return slope\n\n    def __HalfSeries(self, s: list, n: int) -> list:\n        X = []\n        for i in range(0, len(s) - 1, 2):\n            X.append((s[i] + s[i + 1]) / 2)\n        # if length(s) is odd\n        if len(s) % 2 != 0:\n            X.append(s[-1])\n            n = (n - 1) // 2\n        else:\n            n = n // 2\n        return [np.array(X), n]\n\n    def RS4Hurst(self, ts: np.ndarray, minimal=4, method='L2') -> float:\n        \"\"\"\n        RS Analysis for solve the Hurst exponent.\n        \"\"\"\n        ts = np.array(ts)\n        # N is use for storge the length sequence\n        N, RS, n = [], [], len(ts)\n        while (True):\n            N.append(n)\n            # Calculate the average value of the series\n            m = np.mean(ts)\n            # Construct mean adjustment sequence\n            mean_adj = ts - m\n            # Construct cumulative deviation sequence\n            cumulative_dvi = np.cumsum(mean_adj)\n            # Calculate sequence range\n            srange = max(cumulative_dvi) - min(cumulative_dvi)\n            # Calculate the unbiased standard deviation of this sequence\n            unbiased_std_dvi = np.std(ts, ddof=1)\n            # Calculate the rescaled range of this sequence\n            RS.append(srange / unbiased_std_dvi)\n            # While n < 2 then break\n            if n < minimal:\n                break\n            # Rebuild this sequence by half length\n            ts, n = self.__HalfSeries(ts, n)\n        # Get Hurst-index by fit log(RS)~log(n)\n        slope = np.polyfit(np.log(N), np.log(RS), deg = 1)[0]\n        # slope = self.__FitCurve(N, RS, method=method)\n        return slope\n\n    def EstHurstRSAnalysis(self, ts, minimal=20, IsRandom=False,\n                           method='L2') -> float:\n        '''\n        RS Calculate the Hurst exponent using DFA analysis.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 50.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Rescaled Range Analysis (DFA).\n\n        References\n        ----------\n        [1] H.-H.Amjad et al. (2021) Comparison of Hurst exponent estimation\n        methods, Physical Review E 49(2), 1685-1689.\n        [2] R.Weron (2002) Estimating long range dependence: finite sample\n        properties and confidence intervals, Physica A 312, 285-299.\n        [3] E.E.Peters (1994) Fractal Market Analysis, Wiley.\n        [4] A.A.Anis, E.H.Lloyd (1976) The expected value of the adjusted\n        rescaled Hurst range of independent normal summands, Biometrica 63,\n        283-298.\n\n        Written by z.q.feng (2022.09.23).\n        '''\n        N = len(ts)\n\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n\n        # M is use for storge the length sequence\n        RSe, ERS_AL = [], []\n        for m in M:\n            RSm = []\n            k = OptN // m\n            X = np.reshape(ts[N - OptN:], [k, m])\n            for Xm in X:\n                # Calculate the average value of the sub-series\n                Em = np.mean(Xm)\n                # Construct mean adjustment sequence\n                mean_adj = Xm - Em\n                # Construct cumulative deviation sequence\n                cumulative_dvi = np.cumsum(mean_adj)\n                # Calculate sequence range\n                srange = max(cumulative_dvi) - min(cumulative_dvi)\n                # Calculate the unbiased standard deviation of this sequence\n                unbiased_std_dvi = np.std(mean_adj, ddof=1)\n                # Calculate the rescaled range of this sequence under n length\n                RSm.append(srange / unbiased_std_dvi)\n            RSe.append(np.mean(RSm))\n\n        # Compute Anis-Lloyd[4] and Peters[3] corrected theoretical E(R/S)\n        for m in M:\n            # (see [2] for details)\n            K = np.linspace(1, m - 1, m - 1)\n            ratio = (m - 0.5) / m * np.sum(((-K + m) / K)**0.5)\n            if m > 340:\n                ERS_AL.append(ratio / (0.5 * pi * m)**0.5)\n            else:\n                ERS_AL.append((gamma(0.5*(m-1))*ratio)/(gamma(0.5*m)*pi**0.5))\n        # see Peters[3] page 66 eq 5.1\n        ERS = (0.5 * pi * np.array(M))**0.5\n\n        RSe, ERS_AL = np.array(RSe), np.array(ERS_AL)\n        RS = RSe - ERS_AL + ERS if IsRandom else RSe\n\n        slope = self.__FitCurve(M, RS, method=method)\n        return slope\n\n    def EstHurstRSAnalysis2(self, ts, minimal=20, method='L2') -> float:\n        '''\n        RS Calculate the Hurst exponent using DFA analysis.\n\n        Parameters\n        ----------\n        ts     : Time series.\n        minimal: The box sizes that the sample is divided into, default as 50.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        The Hurst exponent of time series X using\n        Rescaled Range Analysis (DFA).\n\n        References\n        ----------\n        [1] H.-H.Amjad et al. (2021) Comparison of Hurst exponent estimation\n        methods, Physical Review E 49(2), 1685-1689.\n        [2] R.Weron (2002) Estimating long range dependence: finite sample\n        properties and confidence intervals, Physica A 312, 285-299.\n        [3] E.E.Peters (1994) Fractal Market Analysis, Wiley.\n        [4] A.A.Anis, E.H.Lloyd (1976) The expected value of the adjusted\n        rescaled Hurst range of independent normal summands, Biometrica 63,\n        283-298.\n\n        Written by z.q.feng (2022.09.23).\n        '''\n        N = len(ts) + 1\n\n        OptN = super().findOptN(N, minimal=minimal)\n        M = super().Divisors(OptN, minimal=minimal)\n        y = np.concatenate([[0], np.cumsum(ts)])\n\n        # M is use for storge the length sequence\n        RSe = []\n        for m in M:\n            RSm = []\n            k = OptN // m\n            X = np.reshape(y[N - OptN:], [k, m])\n            for i in range(k):\n                # Calculate sequence range\n                srange = max(X[i]) - min(X[i])\n                # Calculate the unbiased standard deviation of this sequence\n                unbiased_std_dvi = np.std(ts[i*m:(i+1)*m], ddof=0)\n                # Calculate the rescaled range of this sequence under n length\n                RSm.append(srange / unbiased_std_dvi)\n            RSe.append(np.mean(RSm))\n\n        slope = self.__FitCurve(M, RSe, method=method)\n        return slope\n\n    def EstHurstPeriodogram(self, ts, cutoff=0.3, method='L2') -> float:\n        \"\"\"\n        Parameters\n        ----------\n        ts     : Time series.\n        cutoff : Level of low Fourier frequencies, default as 0.5.\n        method : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        Hurst exponent H of a time series ts estimated using the\n        Geweke-Porter-Hudak (GPH, 1983) spectral estimator for periods\n        lower than max(period)^CUTOFF, where CUTOFF=0.5.\n\n        References\n        ----------\n        [1] J.Geweke, S.Porter-Hudak (1983) The estimation and application of\n        long memory time series models, Journal of Time Series Analysis 4,\n        221-238.\n        [2] R.Weron (2002) Estimating long range dependence: finite sample\n        properties and confidence intervals, Physica A 312, 285-299.\n\n        Written by z.q.feng (2022.09.21).\n        \"\"\"\n        N = len(ts)\n        # Compute the Fourier transform of the data\n        # Remove the first component of Y, which is simply the sum of the data\n        Y = fft.fft(ts)[1:N // 2 + 1]\n        # Define the frequencies\n        freq = np.linspace(1 / N, 0.5, N // 2)\n        # Find the low Fourier frequencies\n        index = freq < 1 / N ** cutoff\n        # The periodogram is deﬁned as\n        IL = 4 * np.sin(freq[index] / 2) ** 2\n        # Compute the power as the squared absolute value of Y\n        # A plot of power versus frequency is the 'periodogram'\n        power = abs(Y[index]) ** 2 / N\n        slope = self.__FitCurve(IL, power, method=method)\n        return 0.5 - slope / 2\n\n    def EstHurstAWC(self, ts, wavelet=\"db24\", wavemode=\"periodization\",\n                    method='L2') -> float:\n        \"\"\"\n        Parameters\n        ----------\n        ts      : Time series.\n        wavelet : Wavelet type, default as \"db24\".\n        wavemode: The discrete wavelet transform extension mode,\n                  default as \"periodization\".\n        method  : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        Hurst exponent H of a time series ts estimated using the\n        Average Wavelet Coefficient (AWC) method.\n\n        References\n        ----------\n        [1] I.Simonsen, A.Hansen, O.Nes (1998) Determination of the Hurst\n        exponent by use of wavelet transforms, Physical Review E 58, 2779-2787.\n        [2] I.Simonsen (2003) Measuring anti-correlations in the Nordic\n        electricity spot market by wavelets, Physica A 322, 597-606.\n        [3] R.Weron, I.Simonsen, P.Wilman (2004) Modeling highly volatile and\n        seasonal markets: evidence from the Nord Pool electricity market, in\n        \"The Application of Econophysics\", ed. H. Takayasu, Springer, 182-191.\n        [4] R.Weron (2006) Modeling and Forecasting Electricity Loads and\n        Prices: A Statistical Approach, Wiley, Chichester.\n\n        Written by z.q.feng (2022.09.24).\n        Based on function awc_hurst.m originally written by Rafal Weron\n        (2014.06.21).\n        \"\"\"\n        # Do not allow for too large values of N\n        N = int(np.floor(np.log2(len(ts)))) - 1\n\n        # Daubechies wavelet of order 24\n        # Set the DWT mode to periodization, see pywt.Modes for details\n        # coeffs contains one Approximation and N Detail-Coefficients\n        coeffs = pywt.wavedec(ts, wavelet=wavelet, mode=wavemode, level=N)\n        sc, awc = [], []\n        for i in range(1, N - 1):\n            # Get the Detail-Coefficients\n            cD = coeffs[-i]\n            sc.append(2 ** i)\n            # Compute the AWC statistics\n            awc.append(np.mean(abs(cD)))\n        # Level value of N is too high\n        # sc, awc = sc[:-1], awc[:-1]\n        slope = self.__FitCurve(sc, awc, method=method)\n        return slope + 0.5\n\n    def EstHurstVVL(self, ts, wavelet=\"haar\", wavemode=\"periodization\",\n                    method='L2') -> float:\n        \"\"\"\n        Parameters\n        ----------\n        ts      : Time series. Be careful of N >= 2^15.\n        wavelet : Wavelet type, default as \"db24\".\n        wavemode: The discrete wavelet transform extension mode,\n                  default as \"periodization\".\n        method  : The method to fit curve, default as minimal l2-norm.\n\n        Returns\n        -------\n        Hurst exponent H of a time series ts estimated using the\n        Variance Versus Level Method (VVL) using wavelets.\n\n        References\n        ----------\n        [1] Hamza A H, Hmood M Y. Comparison of Hurst exponent estimation\n        methods[J]. 2021.\n\n        Written by z.q.feng (2022.09.24).\n        \"\"\"\n        # Do not allow for too large values of N\n        N = int(np.floor(np.log2(len(ts))))\n        # Haar wavelet\n        # Set the DWT mode to periodization, see pywt.Modes for details\n        # coeffs contains one Approximation and N Detail-Coefficients\n        coeffs = pywt.wavedec(ts, wavelet, level=N, mode=wavemode)\n        sc, vvl = [], []\n        for i in range(1, N - 1):\n            # Get the Detail-Coefficients\n            cD = coeffs[-i]\n            sc.append(2 ** i)\n            # Compute the VVL statistics\n            vvl.append(np.var(abs(cD), ddof=1))\n        slope = self.__FitCurve(sc, vvl, method=method)\n        return (slope + 1) / 2\n\n    def EstHurstLocalWhittle(self, ts: np.ndarray) -> float:\n        \"\"\"\n        Semiparametric Gaussian Estimation via Fast Fourier transform.\n\n        Parameters\n        ----------\n        ts        : Time series.\n        iter_count: Accuracy for estimation degree.\n\n        Returns\n        -------\n        The Hurst exponent of time series ts.\n\n        References\n        ----------\n        [1] Robinson, P. M.(1995). \"Gaussian semiparametric estimation\n            of long-range dependence\". The Annals of Statistics. 23(5):\n            1630–1661. doi:10.1214/aos/1176324317.\n\n        Written by z.q.feng (2023.06.07).\n        \"\"\"\n        n = len(ts)\n        m = n // 2  # Less than n / 2\n        w = fft.fft(ts)[1:m + 1]  # Fast Fourier transform\n        Periodogram = abs(w)**2\n        Freqs = np.linspace(1, m + 1, m) / n  # Frequences\n\n        def Whittle_target(H, **kwargs):\n            \"\"\"\n            Target function to minimize in Local Whittle method.\n            \"\"\"\n            Freqs = kwargs[\"Freqs\"]\n            Periodogram = kwargs[\"Periodogram\"]\n            gH = np.mean(Freqs**(2 * H - 1) * Periodogram)\n            rH = np.log(gH) - (2 * H - 1) * np.mean(np.log(Freqs))\n            return rH\n\n        H = super().LocalMin(\n                Whittle_target,\n                [0.001, 0.999],\n                Periodogram=Periodogram,\n                Freqs=Freqs\n            )\n\n        return H\n\n    def EstHurstLSSD(self, ts: np.ndarray, max_scale: int, p=6, q=0,\n                     eps=1e-6) -> float:\n        \"\"\"\n        Least Squares based on Standard Deviation. The fitting error is\n        constructed by sample standard deviation.\n\n        Parameters\n        ----------\n        ts        : Time series.\n        max_scale : Maximum aggregation scale(>= length / 10).\n        p         : Parameter used to determine the weights.\n        q         : Parameter used to determine the penalty factor.\n        eps       : Accuracy for estimation.\n\n        Returns\n        -------\n        The Hurst exponent of time series ts.\n\n        References\n        ----------\n        [1] Koutsoyiannis D (2003) Climate change, the Hurst phenomenon, and\n            hydrological statistics. Hydrological Sciences Journal 48(1):3–24.\n            doi:10.1623/hysj.48.1.3.43481.\n        [2] Tyralis H, Koutsoyiannis D (2011) Simultaneous estimation of the\n            parameters of the Hurst-Kolmogorov stochastic process. Stochastic\n            Environmental Research & Risk Assessment 25(1):21–33.\n            doi:10.1007/s004770100408x.\n\n        Written by z.q.feng (2023.06.07).\n        \"\"\"\n        n = len(ts)\n        # This maximum value was chosen so that VarSeq can be estimated\n        # from at least 10 data values.\n        max_scale = min(max_scale, n // 10 + 1)\n        stdSeq = np.linspace(1, max_scale, max_scale)\n        # {1, 2, ..., max_scale}\n        kscale = np.linspace(1, max_scale, max_scale).astype(int)\n        # Unbiased Stadnard Deviation of each sample\n        for scale in kscale:\n            sample = np.sum(\n                        np.reshape(ts[n % scale:], [n // scale, scale]),\n                        axis=1\n                        )\n            stdSeq[scale - 1] = np.std(sample, ddof=1)\n\n        def LSSDIterFun(H, **kwargs):\n            \"\"\"\n            A constructive mapping in LSSD method. Each improved estimate can\n            reduce the fitting error and continues this way until convergence.\n            \"\"\"\n            n = kwargs[\"n\"]\n            p = kwargs[\"p\"]\n            q = kwargs[\"q\"]\n            kscale = kwargs[\"kscale\"]\n            stdSeq = kwargs[\"stdSeq\"]\n            f = n / kscale\n            kp = kscale**p\n            logk = np.log(kscale)\n            # eq.A.3 in Ref[1]\n            a1 = np.sum(1 / kp)\n            a2 = np.sum(logk / kp)\n            # eq.12 in Ref[2]\n            ckH = ((f - f**(2 * H - 1)) / (f - 0.5))**0.5\n            # eq.A.5 in Ref[1]\n            dkH = logk + np.log(f) / (1 - f**(2 - 2 * H))\n            # eq.A.4 in Ref[1]\n            aH_1 = np.sum(dkH / kp)\n            aH_2 = np.sum(dkH * logk / kp)\n            bH_1 = np.sum((np.log(stdSeq) - np.log(ckH)) / kp)\n            bH_2 = np.sum(dkH * (np.log(stdSeq) - np.log(ckH)) / kp)\n            g1 = a1 * bH_2 - aH_1 * bH_1\n            g2 = a1 * aH_2 - aH_1 * a2\n            return (g1 if q == 0 else (g1 - a1 * H**q)) / g2\n\n        # Solving the fixed point\n        H = super().FixedPointSolver(\n                LSSDIterFun, 0.5,\n                n=n, p=p, q=q,\n                kscale=kscale,\n                stdSeq=stdSeq\n            )\n\n        return H\n\n    def EstHurstLSV(self, ts: np.ndarray, max_scale: int, p=6, q=0) -> float:\n        \"\"\"\n        Least Squares based on Variance. The fitting error is constructed by\n        sample variances.\n\n        Parameters\n        ----------\n        ts        : Time series.\n        max_scale : Maximum aggregation scale(>= length / 10).\n        p         : Parameter used to determine the weights.\n        q         : Parameter used to determine the penalty factor.\n\n        Returns\n        -------\n        The Hurst exponent of time series ts.\n\n        References\n        ----------\n        [1] Tyralis H, Koutsoyiannis D (2011) Simultaneous estimation of the\n            parameters of the Hurst-Kolmogorov stochastic process. Stochastic\n            Environmental Research & Risk Assessment 25(1):21–33.\n            doi:10.1007/s004770100408x.\n\n        Written by z.q.feng (2023.06.07).\n        \"\"\"\n        n = len(ts)\n        # This maximum value was chosen so that VarSeq can be estimated\n        # from at least 10 data values.\n        max_scale = min(max_scale, n // 10 + 1)\n        varSeq = np.linspace(1, max_scale, max_scale)\n        # {1, 2, ..., max_scale}\n        kscale = np.linspace(1, max_scale, max_scale).astype(int)\n        # Variance of each sample\n        for scale in kscale:\n            sample = np.sum(\n                np.reshape(ts[n % scale:], [n // scale, scale]),\n                axis=1\n            )\n            varSeq[scale - 1] = np.var(sample, ddof=1)\n\n        def LSV_target(H: float, **kwargs) -> float:\n            \"\"\"\n            Target function to minimize in LSV method.\n            \"\"\"\n            n = kwargs[\"n\"]\n            p = kwargs[\"p\"]\n            q = kwargs[\"q\"]\n            kscale = kwargs[\"kscale\"]\n            varSeq = kwargs[\"varSeq\"]\n            f = n / kscale\n            kp = kscale**p\n            # Left side of eq.22 in Ref[1]\n            d1 = np.sum(varSeq**2 / kp)\n            # eq.17 in Ref[1]\n            ckH = (f - f**(2 * H - 1)) / (f - 1)\n            # eq.20 in Ref[1]\n            a1H = np.sum((ckH**2 * kscale**(4 * H)) / kp)\n            a2H = np.sum((ckH * kscale**(2 * H) * varSeq) / kp)\n            r = d1 - a2H**2 / (a1H if a1H > 1e-8 else 1e-8)\n            # In Ref[1], to avoid values of sigma tending to infinity\n            # when H->1, penalty factor [H^(q+1)]/(q+1) for a high q(near 50)\n            # is added to target function, see eq.24.\n            return r if q == 0 else r + H**(q + 1) / (q + 1)\n\n        # TODO: Choose an appropriate optimize algorithm.\n        H = super().LocalMin(\n                LSV_target,\n                [0.001, 0.999],\n                n=n, p=p, q=q,\n                kscale=kscale,\n                varSeq=varSeq\n            )\n\n        return H\n\n    def __GHESample(self, ts, tau):\n        \"\"\"\n        Generates sample sequence as |X(t+τ)-X(t)| for t in {0, 1, ..., N-τ-1}.\n        \"\"\"\n        return np.abs(ts[tau:] - ts[:-tau])\n\n    def EstHurstGHE(self, ts, q=2, method=\"L2\"):\n        \"\"\"\n        Generalized Hurst Exponent. Estimated the Hurst exponent by using the\n        qth-order moments of the distribution of the increments.\n\n        Parameters\n        ----------\n        ts : Time sequence.\n        q  : Moments order.\n\n        Returns\n        -------\n        Hurst exponent of the time sequence\n\n        References\n        ----------\n        [1] Albert-László Barabási and Tamás Vicsek. Multifractality of self-\n            affine fractals. Physical review A, 44(4):2730, 1991.\n        [2] Tiziana Di Matteo, Tomaso Aste, and Michel M Dacorogna. Scaling\n            behaviors in diﬀerently developed markets. Physica A: statistical\n            mechanics and its applications, 324(1-2):183–188, 2003.\n        [3] A Gómez-Águila, JE Trinidad-Segovia, and MA Sánchez-Granero.\n            Improvement in hurst exponent estimation and its application to\n            ﬁnancial markets. Financial Innovation, 8(1):1–21, 2022.\n        \"\"\"\n        Y = np.concatenate([[0], np.cumsum(ts)])\n        K, Tau = [], [i for i in range(1, 5)]\n        for tau in Tau:\n            # see Ref[3] for details.\n            K.append(np.mean(self.__GHESample(Y, tau)**q))\n        # k_q(τ) \\propto τ^{qH(q)} where H(q) is the generalized Hurst exponent\n        slope = self.__FitCurve(Tau, K, method=method)\n        return np.clip( slope / q, 0.0000001, 0.99999999)\n\n    def EstHurstKS(self, ts):\n        \"\"\"\n        Kolmogorov-Smirnov Method. In most cases, existing methods use the\n        scaling behavior (a power law) of certain elements of the process,\n        this method take expected values of the equality in distribution to\n        estimate the Hurst exponent. However, equality in distribution is a\n        stronger concept than that in expected values.\n\n        Parameters\n        ----------\n        ts : Time sequence.\n\n        Returns\n        -------\n        Hurst exponent of the time sequence.\n\n        References\n        ----------\n        [1] A Gómez-Águila, JE Trinidad-Segovia, and MA Sánchez-Granero.\n            Improvement in hurst exponent estimation and its application to\n            ﬁnancial markets. Financial Innovation, 8(1):1–21, 2022.\n        [2] JL Hodges Jr. The signiﬁcance probability of the smirnov two-sample\n            test. Arkiv för matematik, 3(5):469–486, 1958.\n        \"\"\"\n        N = len(ts) + 1\n        Y = np.concatenate([[0], np.cumsum(ts)])\n        scaling_range = [2**i for i in range(int(np.log2(N)) - 2)]\n        t0 = self.__GHESample(Y, scaling_range[0])\n\n        def KS_target(H, t0):\n            # see Ref[2] for details of Kolmogorov-Smirnov test.\n            return np.sum(\n                [stats.ks_2samp(t0, self.__GHESample(Y, tau)/tau**H).statistic\n                 for tau in scaling_range[1:]]\n            )\n\n        H = super().LocalMin(KS_target, [0.001, 0.999], t0=t0)\n        return H\n\n    def EstHurstTTA(self, ts, max_scale=11, method=\"L2\"):\n        \"\"\"\n        Triangle Total Areas Method. Hurst exponent is the possibility of high\n        or low values occurrence in time sequences. Based on this fact,\n        triangle area of three samples in time sequences can be an important\n        parameter for evaluation of series values. If three samples in time\n        sequences have the same values, then the area of the mated triangle of\n        this three samples has the lowest value.\n\n        Parameters\n        ----------\n        ts        : Time sequence.\n        max_scale : Maximum interval time for each triangle.\n\n        Returns\n        -------\n        Hurst exponent of the time sequence.\n\n        References\n        ----------\n        [1] Hamze Lotfalinezhad and Ali Maleki. TTA, a new approach to\n            estimate hurst exponent with less estimation error and computa-\n            tional time. Physica A: statistical mechanics and its applications,\n            553:124093, 2020.\n        [2] A Gómez-Águila and MA Sánchez-Granero. A theoretical framework\n            for the tta algorithm. Physica A: Statistical Mechanics and its\n            Applications, 582:126288, 2021.\n        \"\"\"\n        Y = np.concatenate([[0], np.cumsum(ts)])\n        # τ is the interval points between first, middle and last point\n        ST, Tau = [], [i for i in range(1, max_scale)]\n        for tau in Tau:\n            area = []\n            # Area is the half of absulote determinant of matrix below:\n            # | i,           ts[i],           1 |\n            # | i + tau,     ts[i + tau],     1 |\n            # | i + 2 * tau, ts[i + 2 * tau], 1 |\n            for i in range(0, len(Y) - 2 * tau, 2 * tau):\n                area.append(\n                    abs(Y[i + 2 * tau] - 2 * Y[i + tau] + Y[i])\n                )\n            ST.append(0.5 * tau * np.sum(area))\n        slope = self.__FitCurve(Tau, ST, method=method)\n        return slope\n\n    def EstHurstTA(self, ts, q=2, method=\"L2\"):\n        \"\"\"\n        Triangle Areas Method. the modiﬁcation is just to consider the\n        distribution of the area of the triangles, instead of the distribution\n        of the sum of the areas of all the triangles.\n\n        Parameters\n        ----------\n        ts : Time sequence.\n        q  : Moments order.\n\n        Returns\n        -------\n        Hurst exponent of the time sequence.\n\n        References\n        ----------\n        [1] A Gómez-Águila and MA Sánchez-Granero. A theoretical framework\n            for the tta algorithm. Physica A: Statistical Mechanics and its\n            Applications, 582:126288, 2021.\n        \"\"\"\n        Y = np.concatenate([[0], np.cumsum(ts)])\n        ST, Tau = [], [2**i for i in range(int(np.log2(len(ts))) - 1)]\n\n        # TODO: Find the correct distribution.\n        for tau in Tau:\n            area = (0.5 * tau * abs(Y[2 * tau] - 2 * Y[tau] + Y[0]))**q\n            ST.append(area)\n\n        slope = self.__FitCurve(Tau, ST, method=method)\n\n        return slope / q - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.566891Z","iopub.execute_input":"2025-06-01T17:15:59.567285Z","iopub.status.idle":"2025-06-01T17:15:59.640495Z","shell.execute_reply.started":"2025-06-01T17:15:59.567251Z","shell.execute_reply":"2025-06-01T17:15:59.639139Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nfrom tqdm.auto import tqdm\nfrom stochastic.processes.noise import FractionalGaussianNoise as FGN\n# from matplotlib import rc\n# rc('text', usetex=True)\nimport numpy as np\n\nHSolver = HurstIndexSolver()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.643064Z","iopub.execute_input":"2025-06-01T17:15:59.643421Z","iopub.status.idle":"2025-06-01T17:15:59.945397Z","shell.execute_reply.started":"2025-06-01T17:15:59.643391Z","shell.execute_reply":"2025-06-01T17:15:59.944259Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\"\"\"\nThis module have been implemented to compute an Hurst exponent\nEstimator using maximum of likelyhood on the periodogram.\nIt is not a log regression as in Hest_Welp\nthe  periodogram is the one given by the scipy library\nthese functions hab been made according to whittlenew,\nwittlefunc and fspecFGN of Matlab Biyu_code\n\"\"\"\nimport numpy as np\nimport scipy.optimize as so\nfrom math import gamma\n\n\ndef whittle(data):\n    \"\"\"This function compute the Hurst exponent of a signal using\n    a maximum of likelihood on periodogram\n    \"\"\"\n    nbpoints = len(data)\n    nhalfm = int((nbpoints - 1) / 2)\n    tmp = np.abs(np.fft.fft(data))\n    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n    return so.fminbound(func, 0, 1)\n\n\ndef whittle_t(data, pointeur, idx):\n    \"\"\"This function compute the Hurst exponent of a signal using\n    a maximum of likelihood on periodogram\n    the return happend in pointeur[idx]\n    \"\"\"\n    nbpoints = len(data)\n    nhalfm = int((nbpoints - 1) / 2)\n    tmp = np.abs(np.fft.fft(data))\n    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n    pointeur[idx] = so.fminbound(func, 0, 1)\n\n\ndef whittle_s(data, idx):\n    \"\"\"This function compute the Hurst exponent of a signal data[idx,:]\n    using a maximum of likelihood on welch periodogram\n    \"\"\"\n    dataidx = data[idx, :]\n    nbpoints = len(dataidx)\n    nhalfm = int((nbpoints - 1) / 2)\n    tmp = np.abs(np.fft.fft(dataidx))\n    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n    return so.fminbound(func, 0, 1)\n\n\ndef whittle_norm_s(data, idx):\n    \"\"\"This function compute the Hurst exponent of a signal data[idx,:]\n    using a maximum of likelihood on periodogram\n    the data are normalized which does'nt change anything !\n    \"\"\"\n    dataidx = data[idx, :]\n    nbpoints = len(dataidx)\n    datanorm = (dataidx - np.mean(dataidx)) / np.var(dataidx)\n    nhalfm = int((nbpoints - 1) / 2)\n    tmp = np.abs(np.fft.fft(datanorm))\n    gammahat = np.exp(2 * np.log(tmp[1:nhalfm + 1])) / (2 * np.pi * nbpoints)\n    func = lambda Hurst: whittlefunc(Hurst, gammahat, nbpoints)\n    return so.fminbound(func, 0, 1)\n\n\ndef whittlefunc(hurst, gammahat, nbpoints):\n    \"\"\"This is the Whittle function\n    \"\"\"\n    gammatheo = fspec_fgn(hurst, nbpoints)\n    qml = gammahat / gammatheo\n    return 2 * (2 * np.pi / nbpoints) * np.sum(qml)\n\n\ndef fspec_fgn(hest, nbpoints):\n    \"\"\"This is the spectral density of a fGN of Hurst exponent hest\n    \"\"\"\n    hhest = - ((2 * hest) + 1)\n    const = np.sin(np.pi * hest) * gamma(- hhest) / np.pi\n    nhalfm = int((nbpoints - 1) / 2)\n    dpl = 2 * np.pi * np.arange(1, nhalfm + 1) / nbpoints\n    fspec = np.ones(nhalfm)\n    for i in np.arange(0, nhalfm):\n        dpfi = np.arange(0, 200)\n        dpfi = 2 * np.pi * dpfi\n        fgi = (np.abs(dpl[i] + dpfi)) ** hhest\n        fhi = (np.abs(dpl[i] - dpfi)) ** hhest\n        dpfi = fgi + fhi\n        dpfi[0] = dpfi[0] / 2\n        dpfi = (1 - np.cos(dpl[i])) * const * dpfi\n        fspec[i] = np.sum(dpfi)\n    fspec = fspec / np.exp(2 * np.sum(np.log(fspec)) / nbpoints)\n    return fspec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.946629Z","iopub.execute_input":"2025-06-01T17:15:59.947373Z","iopub.status.idle":"2025-06-01T17:15:59.962532Z","shell.execute_reply.started":"2025-06-01T17:15:59.947338Z","shell.execute_reply":"2025-06-01T17:15:59.961271Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Generate AnDi-2020 Dataset","metadata":{}},{"cell_type":"code","source":"traj_length = 25\n\nX25, Y25, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n\ntraj_length = 65\n\nX65, Y65, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n\ntraj_length = 125\n\nX125, Y125, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )\n\ntraj_length = 225\n\nX225, Y225, _, _, _, _ =  challenge_theory_dataset(N = 8000, tasks = [1,], dimensions = [1,], min_T = traj_length, max_T = traj_length+1, )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:15:59.963973Z","iopub.execute_input":"2025-06-01T17:15:59.964393Z","iopub.status.idle":"2025-06-01T17:16:13.579409Z","shell.execute_reply.started":"2025-06-01T17:15:59.964354Z","shell.execute_reply":"2025-06-01T17:16:13.578232Z"}},"outputs":[{"name":"stdout","text":"Creating a dataset for task(s) [1] and dimension(s) [1].\nGenerating dataset for dimension 1.\nCreating a dataset for task(s) [1] and dimension(s) [1].\nGenerating dataset for dimension 1.\nCreating a dataset for task(s) [1] and dimension(s) [1].\nGenerating dataset for dimension 1.\nCreating a dataset for task(s) [1] and dimension(s) [1].\nGenerating dataset for dimension 1.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Preprocess ans predict wit RANDI","metadata":{}},{"cell_type":"code","source":"\nbs25 = randi25.layers[0].input_shape[-1]\nbs65 = randi65.layers[0].input_shape[-1]\nbs125 = randi125.layers[0].input_shape[-1]\nbs225 = randi225.layers[0].input_shape[-1]\n#normalizing the data\ndata25 = data_norm(X25[0],dim=1,task=1)\ndata65 = data_norm(X65[0],dim=1,task=1)\ndata125 = data_norm(X125[0],dim=1,task=1)\ndata225 = data_norm(X225[0],dim=1,task=1)\n#reshaping the data\ndata_rs25 = data_reshape(data25,bs=bs25,dim=1)\ndata_rs65 = data_reshape(data65,bs=bs65,dim=1)\ndata_rs125 = data_reshape(data125,bs=bs125,dim=1)\ndata_rs225 = data_reshape(data225,bs=bs225,dim=1)\n#prediction on trajectories of length 200 using a net trained on traj of length \npred_25 = randi25.predict(data_rs25)\npred_65 = randi65.predict(data_rs65)\npred_125 = randi65.predict(data_rs125)\npred_225 = randi65.predict(data_rs225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:13.580863Z","iopub.execute_input":"2025-06-01T17:16:13.581277Z","iopub.status.idle":"2025-06-01T17:16:41.182265Z","shell.execute_reply.started":"2025-06-01T17:16:13.581242Z","shell.execute_reply":"2025-06-01T17:16:41.181032Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Calculate MAE with scikit-learn\nmae_randi_25 = mean_absolute_error(pred_25.flatten(), Y25[0])\nmae_randi_65 = mean_absolute_error(pred_65.flatten(), Y65[0])\nmae_randi_125 = mean_absolute_error(pred_125.flatten(), Y125[0])\nmae_randi_225 = mean_absolute_error(pred_225.flatten(), Y225[0])\nprint(\"RANDI25 MAE :\", mae_randi_25)\nprint(\"RANDI65 MAE :\", mae_randi_65)\nprint(\"RANDI125 MAE :\", mae_randi_125)\nprint(\"RANDI225 MAE :\", mae_randi_225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:41.183511Z","iopub.execute_input":"2025-06-01T17:16:41.183884Z","iopub.status.idle":"2025-06-01T17:16:41.204434Z","shell.execute_reply.started":"2025-06-01T17:16:41.183839Z","shell.execute_reply":"2025-06-01T17:16:41.203253Z"}},"outputs":[{"name":"stdout","text":"RANDI25 MAE : 0.36436835323758426\nRANDI65 MAE : 0.2836006807379425\nRANDI125 MAE : 0.25237917996905745\nRANDI225 MAE : 0.2564756925415248\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Predict with TA-MSD","metadata":{}},{"cell_type":"code","source":"cumdata25 = np.array(X25[0]).reshape(-1,1,25).transpose(0,2,1)\ncumdata65 = np.array(X65[0]).reshape(-1,1,65).transpose(0,2,1)\ncumdata125 = np.array(X125[0]).reshape(-1,1,125).transpose(0,2,1)\ncumdata225 = np.array(X225[0]).reshape(-1,1,225).transpose(0,2,1)\nprint(cumdata25.shape, cumdata65.shape, cumdata125.shape, cumdata225.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:41.205603Z","iopub.execute_input":"2025-06-01T17:16:41.205934Z","iopub.status.idle":"2025-06-01T17:16:41.446930Z","shell.execute_reply.started":"2025-06-01T17:16:41.205906Z","shell.execute_reply":"2025-06-01T17:16:41.445789Z"}},"outputs":[{"name":"stdout","text":"(8000, 25, 1) (8000, 65, 1) (8000, 125, 1) (8000, 225, 1)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"tamsd25 = ta_msd().get_exponent(cumdata25,[1,2,3,4], method='MSD') \ntamsd65 = ta_msd().get_exponent(cumdata65,[1,2,3,4], method='MSD') \ntamsd125 = ta_msd().get_exponent(cumdata125,[1,2,3,4], method='MSD') \ntamsd225 = ta_msd().get_exponent(cumdata225,[1,2,3,4], method='MSD') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:41.448233Z","iopub.execute_input":"2025-06-01T17:16:41.448562Z","iopub.status.idle":"2025-06-01T17:16:58.895297Z","shell.execute_reply.started":"2025-06-01T17:16:41.448527Z","shell.execute_reply":"2025-06-01T17:16:58.894091Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"mae_tamsd25 = mean_absolute_error(tamsd25, Y25[0])\nmae_tamsd65 = mean_absolute_error(tamsd65, Y65[0])\nmae_tamsd125 = mean_absolute_error(tamsd125, Y125[0])\nmae_tamsd225 = mean_absolute_error(tamsd225, Y225[0])\nprint(\"MSD MAE 25 :\", mae_tamsd25)\nprint(\"MSD MAE 65 :\", mae_tamsd65)\nprint(\"MSD MAE 125:\", mae_tamsd125)\nprint(\"MSD MAE 225:\", mae_tamsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:58.896488Z","iopub.execute_input":"2025-06-01T17:16:58.896840Z","iopub.status.idle":"2025-06-01T17:16:58.917309Z","shell.execute_reply.started":"2025-06-01T17:16:58.896798Z","shell.execute_reply":"2025-06-01T17:16:58.915997Z"}},"outputs":[{"name":"stdout","text":"MSD MAE 25 : 0.4185427128473013\nMSD MAE 65 : 0.38742004574948596\nMSD MAE 125: 0.36957214062315535\nMSD MAE 225: 0.3655910550583095\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Predict with HurstEE Layer","metadata":{}},{"cell_type":"code","source":"hurstLayer = HurstEE()\ntfmsd25 = hurstLayer(cumdata25).numpy() * 2\ntfmsd65 = hurstLayer(cumdata65).numpy() * 2\ntfmsd125 = hurstLayer(cumdata125).numpy() * 2\ntfmsd225 = hurstLayer(cumdata225).numpy() * 2\nmae_tfmsd25 = mean_absolute_error(np.nan_to_num(tfmsd25, nan=1.0), Y25[0])\nmae_tfmsd65 = mean_absolute_error(np.nan_to_num(tfmsd65, nan=1.0), Y65[0])\nmae_tfmsd125 = mean_absolute_error(np.nan_to_num(tfmsd125, nan=1.0), Y125[0])\nmae_tfmsd225 = mean_absolute_error(np.nan_to_num(tfmsd225, nan=1.0), Y225[0])\n\n\nprint(\"hurstLayer MAE 25 :\", mae_tfmsd25)\nprint(\"hurstLayer MAE 65 :\", mae_tfmsd65)\nprint(\"hurstLayer MAE 125:\", mae_tfmsd125)\nprint(\"hurstLayer MAE 225:\", mae_tfmsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:16:58.918611Z","iopub.execute_input":"2025-06-01T17:16:58.918992Z","iopub.status.idle":"2025-06-01T17:24:05.807358Z","shell.execute_reply.started":"2025-06-01T17:16:58.918961Z","shell.execute_reply":"2025-06-01T17:24:05.805800Z"}},"outputs":[{"name":"stdout","text":"hurstLayer MAE 25 : 0.41868652768399167\nhurstLayer MAE 65 : 0.3874224408401196\nhurstLayer MAE 125: 0.3697193257465633\nhurstLayer MAE 225: 0.365821585964274\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"pyhurstLayer = PyHurstEE()\npymsd25 = pyhurstLayer(cumdata25).numpy() * 2\npymsd65 = pyhurstLayer(cumdata65).numpy() * 2\npymsd125 = pyhurstLayer(cumdata125).numpy() * 2\npymsd225 = pyhurstLayer(cumdata225).numpy() * 2\nmae_pymsd25 = mean_absolute_error(np.nan_to_num(pymsd25, nan=1.0), Y25[0])\nmae_pymsd65 = mean_absolute_error(np.nan_to_num(pymsd65, nan=1.0), Y65[0])\nmae_pymsd125 = mean_absolute_error(np.nan_to_num(pymsd125, nan=1.0), Y125[0])\nmae_pymsd225 = mean_absolute_error(np.nan_to_num(pymsd225, nan=1.0), Y225[0])\n\n\nprint(\"PyHurstEELayer MAE 25 :\", mae_pymsd25)\nprint(\"PyHurstEELayer MAE 65 :\", mae_pymsd65)\nprint(\"PyHurstEELayer MAE 125:\", mae_pymsd125)\nprint(\"PyHurstEELayer MAE 225:\", mae_pymsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:24:05.808600Z","iopub.execute_input":"2025-06-01T17:24:05.809107Z","iopub.status.idle":"2025-06-01T17:24:09.654687Z","shell.execute_reply.started":"2025-06-01T17:24:05.809072Z","shell.execute_reply":"2025-06-01T17:24:09.653578Z"}},"outputs":[{"name":"stdout","text":"PyHurstEELayer MAE 25 : 0.4186865230651701\nPyHurstEELayer MAE 65 : 0.38742244279480564\nPyHurstEELayer MAE 125: 0.36971932467269014\nPyHurstEELayer MAE 225: 0.3658215900141029\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"methods = ['KS', 'GHE(1)', 'GHE(3)', 'HM', 'MFDFA(2)', 'Whittle', 'TTA', 'PM', 'RS', 'db4', 'LW', 'LSSD', 'LSV', 'GHE(2)', 'TA-MSD', 'TF HurstEE', 'PyHurstEE', 'RANDI']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:24:09.655728Z","iopub.execute_input":"2025-06-01T17:24:09.656109Z","iopub.status.idle":"2025-06-01T17:24:09.660842Z","shell.execute_reply.started":"2025-06-01T17:24:09.656071Z","shell.execute_reply":"2025-06-01T17:24:09.659580Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Estimate Hurst exponent with statistical methods (AnDi-2020 dataset)","metadata":{}},{"cell_type":"code","source":"H25 = [[] for _ in range(len(methods))]\nH65 = [[] for _ in range(len(methods))]\nH125 = [[] for _ in range(len(methods))]\nH225 = [[] for _ in range(len(methods))]\n\nminimal = 3\nmax_scale = 11\n\nfor row in range(len(X25[0])):\n    x25 = np.array(X25[0][row]).reshape(1,-1)\n    ts25=x25[:,1:]-x25[:,:-1]\n    ts25 = ts25 / np.std(ts25)\n    x65 = np.array(X65[0][row]).reshape(1,-1)\n    ts65=x65[:,1:]-x65[:,:-1]\n    ts65 = ts65 / np.std(ts65)\n    x125 = np.array(X125[0][row]).reshape(1,-1)\n    ts125 = x125[:,1:]-x125[:,:-1]\n    ts125 = ts125 / np.std(ts125)\n    x225 = np.array(X225[0][row]).reshape(1,-1)\n    ts225 = x225[:,1:]-x225[:,:-1]\n    ts225 = ts225 / np.std(ts225)\n    \n    H25[0].append(HSolver.EstHurstKS(ts25[0]) *2)\n    H25[1].append(HSolver.EstHurstGHE(ts25[0], q=1, method='L2') *2)\n    H25[2].append(HSolver.EstHurstGHE(ts25[0], q=3, method='L2') *2)\n    H25[3].append(HSolver.EstHurstHiguchi(ts25[0],7, method='L2') *2)\n    # H25[3].append(1)\n    H25[4].append(mf_dfa(ts25[0]) *2)\n    H25[5].append(whittle(ts25[0])  *2)\n    H25[6].append(HSolver.EstHurstTTA(ts25[0],7, method='L2')  *2)\n    H25[7].append(HSolver.EstHurstPeriodogram(ts25[0], cutoff=0.5, method='L2')  *2)\n    H25[8].append(HSolver.EstHurstRSAnalysis2(ts25[0],2, method='L2')  *2)\n    H25[9].append(calculate_Hw(ts25[0])  *2)\n    H25[10].append(HSolver.EstHurstLocalWhittle(ts25[0])  *2)\n    H25[11].append(HSolver.EstHurstLSSD(ts25[0], max_scale) *2)\n    H25[12].append(HSolver.EstHurstLSV(ts25[0], max_scale)  *2)\n    H25[13].append(HSolver.EstHurstGHE(ts25[0], method='L2')  *2)\n        \n    H65[0].append(HSolver.EstHurstKS(ts65[0])  *2)\n    H65[1].append(HSolver.EstHurstGHE(ts65[0], q=1, method='L2') *2)\n    H65[2].append(HSolver.EstHurstGHE(ts65[0],q=3, method='L2')  *2)\n    H65[3].append(HSolver.EstHurstHiguchi(ts65[0], method='L2')  *2)\n    H65[4].append(mf_dfa(ts65[0]) *2)\n    H65[5].append(whittle(ts65[0])  *2)\n    H65[6].append(HSolver.EstHurstTTA(ts65[0],7, method='L2')  *2)\n    H65[7].append(HSolver.EstHurstPeriodogram(ts65[0], cutoff=0.3, method='L2')  *2)\n    H65[8].append(HSolver.EstHurstRSAnalysis2(ts65[0],4, method='L2')  *2)\n    H65[9].append(calculate_Hw(ts65[0])  *2)\n    H65[10].append(HSolver.EstHurstLocalWhittle(ts65[0])  *2)\n    H65[11].append(HSolver.EstHurstLSSD(ts65[0], max_scale)  *2)\n    H65[12].append(HSolver.EstHurstLSV(ts65[0], max_scale)  *2)\n    H65[13].append(HSolver.EstHurstGHE(ts65[0], method='L2')  *2)\n\n\n    H125[0].append(HSolver.EstHurstKS(ts125[0])  *2)\n    H125[1].append(HSolver.EstHurstGHE(ts125[0], q=1, method='L2') *2)\n    H125[2].append(HSolver.EstHurstGHE(ts125[0],q=3, method='L2')  *2)\n    H125[3].append(HSolver.EstHurstHiguchi(ts125[0], method='L2')  *2)\n    H125[4].append(mf_dfa(ts125[0]) *2)\n    H125[5].append(whittle(ts125[0])  *2)\n    H125[6].append(HSolver.EstHurstTTA(ts125[0],7, method='L2')  *2)\n    H125[7].append(HSolver.EstHurstPeriodogram(ts125[0], cutoff=0.3, method='L2')  *2)\n    H125[8].append(HSolver.EstHurstRSAnalysis2(ts125[0],4, method='L2')  *2)\n    H125[9].append(calculate_Hw(ts125[0])  *2)\n    H125[10].append(HSolver.EstHurstLocalWhittle(ts125[0])  *2)\n    H125[11].append(HSolver.EstHurstLSSD(ts125[0], max_scale)  *2)\n    H125[12].append(HSolver.EstHurstLSV(ts125[0], max_scale)  *2)\n    H125[13].append(HSolver.EstHurstGHE(ts125[0], method='L2') *2)\n\n    H225[0].append(HSolver.EstHurstKS(ts225[0])  *2)\n    H225[1].append(HSolver.EstHurstGHE(ts225[0], q=1, method='L2') *2)\n    H225[2].append(HSolver.EstHurstGHE(ts225[0],q=3, method='L2')  *2)\n    H225[3].append(HSolver.EstHurstHiguchi(ts225[0], method='L2')  *2)\n    H225[4].append(mf_dfa(ts225[0]) *2)\n    H225[5].append(whittle(ts225[0])  *2)\n    H225[6].append(HSolver.EstHurstTTA(ts225[0],11, method='L2')  *2)\n    H225[7].append(HSolver.EstHurstPeriodogram(ts225[0], cutoff=0.3, method='L2')  *2)\n    H225[8].append(HSolver.EstHurstRSAnalysis2(ts225[0],4, method='L2')  *2)\n    H225[9].append(calculate_Hw(ts225[0])  *2)\n    H225[10].append(HSolver.EstHurstLocalWhittle(ts225[0])  *2)\n    H225[11].append(HSolver.EstHurstLSSD(ts225[0], max_scale)  *2)\n    H225[12].append(HSolver.EstHurstLSV(ts225[0], max_scale)  *2)\n    H225[13].append(HSolver.EstHurstGHE(ts225[0], method='L2')  *2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:24:09.661876Z","iopub.execute_input":"2025-06-01T17:24:09.662218Z","iopub.status.idle":"2025-06-01T17:58:24.308338Z","shell.execute_reply.started":"2025-06-01T17:24:09.662193Z","shell.execute_reply":"2025-06-01T17:58:24.306877Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Comparison of MAE for various Hurst exponent estimation methods on AnDi-2020 Dataset","metadata":{}},{"cell_type":"code","source":"# MAE H_noised\ndic = {}\ndic['length'] = [25, 65, 125, 225]\nfor i in range(14):\n    dic[methods[i]] = [mean_absolute_error(np.nan_to_num(H25[i], nan=1.0), Y25[0]), mean_absolute_error(np.nan_to_num(H65[i], nan=1.0), Y65[0]),  mean_absolute_error(np.nan_to_num(H125[i], nan=1.0), Y125[0]), mean_absolute_error(np.nan_to_num(H225[i], nan=1.0), Y225[0])]\ndic[methods[14]] = [mae_tamsd25, mae_tamsd65, mae_tamsd125, mae_tamsd225]\ndic[methods[15]] = [mae_tfmsd25, mae_tfmsd65, mae_tfmsd125, mae_tfmsd225]\ndic[methods[16]] = [mae_pymsd25, mae_pymsd65, mae_pymsd125, mae_pymsd225]\ndic[methods[17]] = [mae_randi_25, mae_randi_65, mae_randi_125, mae_randi_225]\nandi1_df = pd.DataFrame(dic)\nandi1_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:58:24.310091Z","iopub.execute_input":"2025-06-01T17:58:24.310401Z","iopub.status.idle":"2025-06-01T17:58:24.648502Z","shell.execute_reply.started":"2025-06-01T17:58:24.310376Z","shell.execute_reply":"2025-06-01T17:58:24.647447Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n0      25  0.454171  0.443984  0.419161  0.452389  0.450180  0.453682   \n1      65  0.376811  0.396477  0.391624  0.384971  0.412330  0.415178   \n2     125  0.356891  0.371977  0.373319  0.363675  0.399313  0.395470   \n3     225  0.335924  0.362133  0.370919  0.335940  0.400207  0.384647   \n\n        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n0  0.732705  0.583806  0.843265  0.619631  0.511131  0.596974  0.527731   \n1  0.583644  0.411059  0.503674  0.484922  0.444520  0.485467  0.447116   \n2  0.523955  0.390735  0.442792  0.436275  0.414037  0.454038  0.419489   \n3  0.451076  0.376583  0.406967  0.413749  0.396574  0.436916  0.405759   \n\n     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n0  0.418598  0.418543    0.418687   0.418687  0.364368  \n1  0.387453  0.387420    0.387422   0.387422  0.283601  \n2  0.369599  0.369572    0.369719   0.369719  0.252379  \n3  0.365614  0.365591    0.365822   0.365822  0.256476  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>KS</th>\n      <th>GHE(1)</th>\n      <th>GHE(3)</th>\n      <th>HM</th>\n      <th>MFDFA(2)</th>\n      <th>Whittle</th>\n      <th>TTA</th>\n      <th>PM</th>\n      <th>RS</th>\n      <th>db4</th>\n      <th>LW</th>\n      <th>LSSD</th>\n      <th>LSV</th>\n      <th>GHE(2)</th>\n      <th>TA-MSD</th>\n      <th>TF HurstEE</th>\n      <th>PyHurstEE</th>\n      <th>RANDI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>0.454171</td>\n      <td>0.443984</td>\n      <td>0.419161</td>\n      <td>0.452389</td>\n      <td>0.450180</td>\n      <td>0.453682</td>\n      <td>0.732705</td>\n      <td>0.583806</td>\n      <td>0.843265</td>\n      <td>0.619631</td>\n      <td>0.511131</td>\n      <td>0.596974</td>\n      <td>0.527731</td>\n      <td>0.418598</td>\n      <td>0.418543</td>\n      <td>0.418687</td>\n      <td>0.418687</td>\n      <td>0.364368</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65</td>\n      <td>0.376811</td>\n      <td>0.396477</td>\n      <td>0.391624</td>\n      <td>0.384971</td>\n      <td>0.412330</td>\n      <td>0.415178</td>\n      <td>0.583644</td>\n      <td>0.411059</td>\n      <td>0.503674</td>\n      <td>0.484922</td>\n      <td>0.444520</td>\n      <td>0.485467</td>\n      <td>0.447116</td>\n      <td>0.387453</td>\n      <td>0.387420</td>\n      <td>0.387422</td>\n      <td>0.387422</td>\n      <td>0.283601</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>125</td>\n      <td>0.356891</td>\n      <td>0.371977</td>\n      <td>0.373319</td>\n      <td>0.363675</td>\n      <td>0.399313</td>\n      <td>0.395470</td>\n      <td>0.523955</td>\n      <td>0.390735</td>\n      <td>0.442792</td>\n      <td>0.436275</td>\n      <td>0.414037</td>\n      <td>0.454038</td>\n      <td>0.419489</td>\n      <td>0.369599</td>\n      <td>0.369572</td>\n      <td>0.369719</td>\n      <td>0.369719</td>\n      <td>0.252379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>225</td>\n      <td>0.335924</td>\n      <td>0.362133</td>\n      <td>0.370919</td>\n      <td>0.335940</td>\n      <td>0.400207</td>\n      <td>0.384647</td>\n      <td>0.451076</td>\n      <td>0.376583</td>\n      <td>0.406967</td>\n      <td>0.413749</td>\n      <td>0.396574</td>\n      <td>0.436916</td>\n      <td>0.405759</td>\n      <td>0.365614</td>\n      <td>0.365591</td>\n      <td>0.365822</td>\n      <td>0.365822</td>\n      <td>0.256476</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# Prepare FBM Dataset based on AnDi-2024 Single-state model","metadata":{}},{"cell_type":"code","source":"#  from https://github.com/AnDiChallenge/\nimport numpy as np\nimport pandas as pd\nfrom stochastic.processes.noise import FractionalGaussianNoise as FGN\nimport warnings\nimport scipy.stats\ndef gaussian(params:list|int, # If list, mu and sigma^2 of the gaussian. If int, we consider sigma = 0\n             size = 1,  # Number of samples to get.\n             bound = None # Bound of the Gaussian, if any.\n            )-> np.array: # Samples from the given Gaussian distribution\n    '''\n    Samples from a Gaussian distribution of given parameters.\n    '''\n    # if we are given a single number, we consider equal to mean and variance = 0\n    if isinstance(params, float) or isinstance(params, int):\n        if size == 1:\n            return params\n        else:\n            return np.array(params).repeat(size)\n    else:\n        mean, var = params        \n        if bound is None:\n            val = np.random.normal(mean, np.sqrt(var), size)\n        if bound is not None:\n            lower, upper = bound\n            if var == 0: \n                if mean > upper or mean < lower:\n                    raise ValueError('Demanded value outside of range.')\n                val = np.ones(size)*mean\n            else:\n                val = scipy.stats.truncnorm.rvs((lower-mean)/np.sqrt(var),\n                                                (upper-mean)/np.sqrt(var),\n                                                loc = mean,\n                                                scale = np.sqrt(var),\n                                                size = size)\n        if size == 1:\n            return val[0]\n        else:\n            return val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:58:24.649605Z","iopub.execute_input":"2025-06-01T17:58:24.650276Z","iopub.status.idle":"2025-06-01T17:58:24.658618Z","shell.execute_reply.started":"2025-06-01T17:58:24.650236Z","shell.execute_reply":"2025-06-01T17:58:24.657293Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Dataset generration from https://github.com/AnDiChallenge/\n\n# %% ../source_nbs/lib_nbs/models_phenom.ipynb 5\nclass models_phenom():\n    def __init__(self):\n        '''\n        This class handles the generation of trajectories from different theoretical models. \n        ''' \n        # We define here the bounds of the anomalous exponent and diffusion coefficient\n        self.bound_D = [1e-12, 1e6]\n        self.bound_alpha = [0, 1.999]\n        \n        # We also define the value in which we consider directed motion\n        self.alpha_directed = 1.9\n        \n        # Diffusion state labels: the position of each type defines its numerical label\n        # i: immobile/trapped; c: confined; f: free-diffusive (normal and anomalous); d: directed\n        self.lab_state = ['i', 'c', 'f', 'd']\n\n# %% ../source_nbs/lib_nbs/models_phenom.ipynb 7\nclass models_phenom(models_phenom):\n    \n    @staticmethod\n    def disp_fbm(alpha : float,\n                 D : float,\n                 T: int, \n                 deltaT : int = 1):\n        ''' Generates normalized Fractional Gaussian noise. This means that, in \n        general:\n        $$\n        <x^2(t) > = 2Dt^{alpha}\n        $$\n                            \n        and in particular:\n        $$\n        <x^2(t = 1)> = 2D \n        $$\n        \n        Parameters\n        ----------\n        alpha : float in [0,2]\n            Anomalous exponent\n        D : float\n            Diffusion coefficient\n        T : int\n            Number of displacements to generate\n        deltaT : int, optional\n            Sampling time\n            \n        Returns\n        -------\n        numpy.array\n            Array containing T displacements of given parameters\n        \n        '''\n        \n        # Generate displacements\n        disp = FGN(hurst = alpha/2).sample(n = T)\n        # Normalization factor\n        disp *= np.sqrt(T)**(alpha)\n        # Add D\n        disp *= np.sqrt(2*D*deltaT)        \n        \n        return disp\n\n# %% ../source_nbs/lib_nbs/models_phenom.ipynb 18\nclass models_phenom(models_phenom):\n    \n    @staticmethod\n    def _constraint_alpha(alpha_1, alpha_2, epsilon_a):\n        ''' Defines the metric for constraining the changes in anomalous\n        exponent'''\n        return alpha_1 - alpha_2 < epsilon_a\n    \n    @staticmethod\n    def _constraint_d(d1, d2, gamma_d):\n        ''' Defines the metric for constraining the changes in anomalous\n        exponent'''\n        if gamma_d < 1:\n            return d2 > d1*gamma_d\n        if gamma_d > 1:\n            return d2 < d1*gamma_d\n    \n    @staticmethod\n    def _sample_diff_parameters(alphas : list, # List containing the parameters to sample anomalous exponent in state (adapt to sampling function)\n                                Ds : list, # List containing the parameters to sample the diffusion coefficient in state (adapt to sampling function).\n                                num_states : int, # Number of diffusive states.\n                                epsilon_a : float, #  Minimum distance between anomalous exponents of various states.\n                                gamma_d : float, # Factor between diffusion coefficient of various states.\n                               ) : \n        '''    \n        Given information of the anomalous exponents (alphas), diffusion coefficients (Ds), the function\n        samples these from a bounded Gaussian distribution with the indicated constraints (epsilon_a,\n        gamma_d). Outputs the list of demanded alphas and Ds.\n        \n        \n        Parameters\n        ----------\n        alphas : list\n        List containing the parameters to sample anomalous exponent in state (adapt to sampling function).\n        Ds : list\n        List containing the parameters to sample the diffusion coefficient in state (adapt to sampling function).\n        num_states : int\n        Number of diffusive states.\n        epsilon_a : float\n        Minimum distance between anomalous exponents of various states.            \n                epsilon workflow: we check val[i] - val[i-1] < epsilon\n                    if you want that val[i] > val[i-1]: epsilon has to be positive\n                    if you want that val[i] < val[i-1]: epsilon has to be negative\n                    if you don't care: epsilon = 0\n                \n        gamma_d : float\n        Factor between diffusion coefficient of various states.            \n                gamma workflow: \n                    for gamma < 1: val[i] < val[i-1]*gamma\n                    for gamma > 1: val[i] > val[i-1]*gamma\n                    for gamma = 1: no check\n        Returns\n        -------\n            :alphas_traj (list): list of anomalous exponents\n            :Ds_traj (list): list of diffusion coefficients\n                      \n        '''\n\n        \n        alphas_traj = []\n        Ds_traj = []\n        for i in range(num_states): \n\n            # for the first state we just sample normally\n            if i == 0:\n                alphas_traj.append(float(gaussian(alphas[i], bound = models_phenom().bound_alpha)))\n                Ds_traj.append(float(gaussian(Ds[i], bound = models_phenom().bound_D)))\n           \n            # For next states we take into account epsilon distance between diffusion\n            # parameter\n            else:\n                ## Checking alpha\n                alpha_state = float(gaussian(alphas[i], bound = models_phenom().bound_alpha))\n                D_state = float(gaussian(Ds[i], bound = models_phenom().bound_D))\n\n                if epsilon_a[i-1] != 0:\n                    idx_while = 0\n                    while models_phenom()._constraint_alpha(alphas_traj[-1], alpha_state, epsilon_a[i-1]):\n                    #alphas_traj[-1] - alpha_state < epsilon_a[i-1]:\n                        alpha_state = float(gaussian(alphas[i], bound = models_phenom().bound_alpha))                        \n                        idx_while += 1\n                        if idx_while > 100: # check that we are not stuck forever in the while loop\n                            raise FileNotFoundError(f'Could not find correct alpha for state {i} in 100 steps. State distributions probably too close.')\n\n                alphas_traj.append(alpha_state)\n                \n                ## Checking D\n                if gamma_d[i-1] != 1:    \n                    \n                    idx_while = 0\n                    while models_phenom()._constraint_d(Ds_traj[-1], D_state, gamma_d[i-1]):\n                        D_state = float(gaussian(Ds[i], bound = models_phenom().bound_D))\n                        idx_while += 1\n                        if idx_while > 100: # check that we are not stuck forever in the while loop\n                            raise FileNotFoundError(f'Could not find correct D for state {i} in 100 steps. State distributions probably too close.')\n               \n    \n                Ds_traj.append(D_state)\n                \n        return alphas_traj, Ds_traj\n\n# %% ../source_nbs/lib_nbs/models_phenom.ipynb 26\nclass models_phenom(models_phenom):\n    \n    @staticmethod\n    def _single_state_traj(T :int = 200, \n                          D : float = 1, \n                          alpha : float = 1, \n                          L : float = None,\n                          deltaT : int = 1,\n                          dim : int = 2):\n        '''\n        Generates a single state trajectory with given parameters. \n        \n        Parameters\n        ----------\n        T : int\n            Length of the trajectory\n        D : float\n            Diffusion coefficient       \n        alpha : float\n            Anomalous exponent\n        L : float\n            Length of the box acting as the environment\n        deltaT : int, optional\n            Sampling time\n        dim : int\n            Dimension of the walk (can be 2 or 3)\n            \n        Returns\n        -------\n        tuple\n            - pos: position of the particle\n            - labels:  anomalous exponent, D and state at each timestep. State is always free here.\n            \n        '''\n        \n        \n        # Trajectory displacements\n        disp_d = []\n        for d in range(dim):\n            disp_d.append(models_phenom().disp_fbm(alpha, D, T))\n        # Labels\n        lab_diff_state = np.ones(T)*models_phenom().lab_state.index('f') if alpha < models_phenom().alpha_directed else np.ones(T)*models_phenom().lab_state.index('d')\n        labels = np.vstack((np.ones(T)*alpha, \n                            np.ones(T)*D,\n                            lab_diff_state\n                           )).transpose()\n\n        # If there are no boundaries\n        if not L:\n            \n            pos = np.vstack([np.cumsum(disp)-disp[0] for disp in disp_d]).transpose()\n            \n            return pos, labels\n\n        # If there are, apply reflecting boundary conditions\n        else:\n            pos = np.zeros((T, dim))\n\n            # Initialize the particle in a random position of the box\n            pos[0, :] = np.random.rand(dim)*L\n            for t in range(1, T):\n                if dim == 2:\n                    pos[t, :] = [pos[t-1, 0]+disp_d[0][t], \n                                 pos[t-1, 1]+disp_d[1][t]]            \n                elif dim == 3:\n                    pos[t, :] = [pos[t-1, 0]+disp_d[0][t], \n                                 pos[t-1, 1]+disp_d[1][t], \n                                 pos[t-1, 2]+disp_d[2][t]]            \n\n\n                # Reflecting boundary conditions\n                while np.max(pos[t, :])>L or np.min(pos[t, :])< 0: \n                    pos[t, pos[t, :] > L] = pos[t, pos[t, :] > L] - 2*(pos[t, pos[t, :] > L] - L)\n                    pos[t, pos[t, :] < 0] = - pos[t, pos[t, :] < 0]\n\n            return pos, labels\n\n# %% ../source_nbs/lib_nbs/models_phenom.ipynb 33\nclass models_phenom(models_phenom):\n    \n    \n    def single_state(self,\n                     N:int = 10,\n                     T:int = 200, \n                     Ds:list = [1, 0], \n                     alphas:list = [1, 0], \n                     L:float = None,\n                     dim:int = 2):\n        '''\n        Generates a dataset made of single state trajectories with given parameters.\n        \n        Parameters\n        ----------\n        N : int, list\n            Number of trajectories in the dataset\n        T : int\n            Length of the trajectory\n        Ds : float\n            If list, mean and variance from which to sample the diffusion coefficient. If float, we consider variance = 0.\n        alphas : float\n            If list, mean and variance from which to sample the anomalous exponent. If float, we consider variance = 0.\n        L : float\n            Length of the box acting as the environment\n        deltaT : int, optional\n            Sampling time            \n        dim : int\n            Dimension of the walk (can be 2 or 3)\n            \n        Returns\n        -------\n        tuple\n            - positions: position of the N trajectories.\n            - labels:  anomalous exponent, D and state at each timestep. State is always free here.         \n        '''\n\n        positions = np.zeros((T, N, dim))\n        labels = np.zeros((T, N, 3))\n\n        for n in range(N):\n            alpha_traj = gaussian(alphas, bound = self.bound_alpha)\n            D_traj = gaussian(Ds, bound = self.bound_D)\n            # Get trajectory from single traj function\n            pos, lab = self._single_state_traj(T = T, \n                                               D = D_traj, \n                                               alpha = alpha_traj, \n                                               L = L,\n                                               dim = dim\n                                              )        \n            positions[:, n, :] = pos\n            labels[:, n, :] = lab\n\n        return positions, labels\n\nmodels_class = models_phenom()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:58:24.659976Z","iopub.execute_input":"2025-06-01T17:58:24.660429Z","iopub.status.idle":"2025-06-01T17:58:24.690060Z","shell.execute_reply.started":"2025-06-01T17:58:24.660398Z","shell.execute_reply":"2025-06-01T17:58:24.688670Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"traj_length = 25\n\nX25, Y25 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n\ntraj_length = 65\n\nX65, Y65 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n\ntraj_length = 125\n\nX125, Y125 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n\ntraj_length = 225\n\nX225, Y225 =  models_class.single_state(N = 8000, T = traj_length, Ds = [1, 0], alphas = [1, 1])\n\nprint(X225.shape)\n\nY25, Y65, Y125, Y225 = [Y25[0,:,0]], [Y65[0,:,0]], [Y125[0,:,0]], [Y225[0,:,0]]\nX25 = [X25[:,:,:1].transpose(1, 2, 0)]\nX65 = [X65[:,:,:1].transpose(1, 2, 0)]\nX125 = [X125[:,:,:1].transpose(1, 2, 0)]\nX225 = [X225[:,:,:1].transpose(1, 2, 0)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:58:24.691252Z","iopub.execute_input":"2025-06-01T17:58:24.691629Z","iopub.status.idle":"2025-06-01T17:58:44.650446Z","shell.execute_reply.started":"2025-06-01T17:58:24.691599Z","shell.execute_reply":"2025-06-01T17:58:44.649348Z"}},"outputs":[{"name":"stdout","text":"(225, 8000, 2)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# RANDI prediction","metadata":{}},{"cell_type":"code","source":"bs25 = randi25.layers[0].input_shape[-1]\nbs65 = randi65.layers[0].input_shape[-1]\nbs125 = randi125.layers[0].input_shape[-1]\nbs225 = randi225.layers[0].input_shape[-1]\n#normalizing the data\ndata25 = data_norm(X25[0],dim=1,task=1)\ndata65 = data_norm(X65[0],dim=1,task=1)\ndata125 = data_norm(X125[0],dim=1,task=1)\ndata225 = data_norm(X225[0],dim=1,task=1)\n#reshaping the data\ndata_rs25 = data_reshape(data25,bs=bs25,dim=1)\ndata_rs65 = data_reshape(data65,bs=bs65,dim=1)\ndata_rs125 = data_reshape(data125,bs=bs125,dim=1)\ndata_rs225 = data_reshape(data225,bs=bs225,dim=1)\n#prediction on trajectories of length 200 using a net trained on traj of length \npred_25 = randi25.predict(data_rs25)\npred_65 = randi65.predict(data_rs65)\npred_125 = randi65.predict(data_rs125)\npred_225 = randi65.predict(data_rs225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:58:44.651514Z","iopub.execute_input":"2025-06-01T17:58:44.651832Z","iopub.status.idle":"2025-06-01T17:59:10.190537Z","shell.execute_reply.started":"2025-06-01T17:58:44.651799Z","shell.execute_reply":"2025-06-01T17:59:10.189365Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Calculate MAE with scikit-learn\nmae_randi_25 = mean_absolute_error(pred_25.flatten(), Y25[0])\nmae_randi_65 = mean_absolute_error(pred_65.flatten(), Y65[0])\nmae_randi_125 = mean_absolute_error(pred_125.flatten(), Y125[0])\nmae_randi_225 = mean_absolute_error(pred_225.flatten(), Y225[0])\nprint(\"RANDI25 MAE :\", mae_randi_25)\nprint(\"RANDI65 MAE :\", mae_randi_65)\nprint(\"RANDI125 MAE :\", mae_randi_125)\nprint(\"RANDI225 MAE :\", mae_randi_225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:59:10.191921Z","iopub.execute_input":"2025-06-01T17:59:10.192341Z","iopub.status.idle":"2025-06-01T17:59:10.203140Z","shell.execute_reply.started":"2025-06-01T17:59:10.192310Z","shell.execute_reply":"2025-06-01T17:59:10.202055Z"}},"outputs":[{"name":"stdout","text":"RANDI25 MAE : 0.3121075043512037\nRANDI65 MAE : 0.2355222448499132\nRANDI125 MAE : 0.2099300476626134\nRANDI225 MAE : 0.2161124641843669\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# TA-MSD prediction","metadata":{}},{"cell_type":"code","source":"cumdata25 = np.array(X25[0]).reshape(-1,1,25).transpose(0,2,1)\ncumdata65 = np.array(X65[0]).reshape(-1,1,65).transpose(0,2,1)\ncumdata125 = np.array(X125[0]).reshape(-1,1,125).transpose(0,2,1)\ncumdata225 = np.array(X225[0]).reshape(-1,1,225).transpose(0,2,1)\nprint(cumdata25.shape, cumdata65.shape, cumdata125.shape, cumdata225.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:59:10.204202Z","iopub.execute_input":"2025-06-01T17:59:10.204535Z","iopub.status.idle":"2025-06-01T17:59:10.234451Z","shell.execute_reply.started":"2025-06-01T17:59:10.204508Z","shell.execute_reply":"2025-06-01T17:59:10.233312Z"}},"outputs":[{"name":"stdout","text":"(8000, 25, 1) (8000, 65, 1) (8000, 125, 1) (8000, 225, 1)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"tamsd25 = ta_msd().get_exponent(cumdata25,[1,2,3,4], method='MSD') \ntamsd65 = ta_msd().get_exponent(cumdata65,[1,2,3,4], method='MSD') \ntamsd125 = ta_msd().get_exponent(cumdata125,[1,2,3,4], method='MSD') \ntamsd225 = ta_msd().get_exponent(cumdata225,[1,2,3,4], method='MSD') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:59:10.235598Z","iopub.execute_input":"2025-06-01T17:59:10.235983Z","iopub.status.idle":"2025-06-01T17:59:27.922356Z","shell.execute_reply.started":"2025-06-01T17:59:10.235952Z","shell.execute_reply":"2025-06-01T17:59:27.921271Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"mae_tamsd25 = mean_absolute_error(tamsd25, Y25[0])\nmae_tamsd65 = mean_absolute_error(tamsd65, Y65[0])\nmae_tamsd125 = mean_absolute_error(tamsd125, Y125[0])\nmae_tamsd225 = mean_absolute_error(tamsd225, Y225[0])\nprint(\"MSD MAE 25 :\", mae_tamsd25)\nprint(\"MSD MAE 65 :\", mae_tamsd65)\nprint(\"MSD MAE 125:\", mae_tamsd125)\nprint(\"MSD MAE 225:\", mae_tamsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:59:27.923469Z","iopub.execute_input":"2025-06-01T17:59:27.923870Z","iopub.status.idle":"2025-06-01T17:59:27.935116Z","shell.execute_reply.started":"2025-06-01T17:59:27.923832Z","shell.execute_reply":"2025-06-01T17:59:27.934021Z"}},"outputs":[{"name":"stdout","text":"MSD MAE 25 : 0.21516067491711058\nMSD MAE 65 : 0.12885394284011328\nMSD MAE 125: 0.0936305952373844\nMSD MAE 225: 0.07114303808403756\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# HurstEE Layer prediction","metadata":{}},{"cell_type":"code","source":"tf_msd = HurstEE()\ntfmsd25 = tf_msd(cumdata25).numpy() * 2\ntfmsd65 = tf_msd(cumdata65).numpy() * 2\ntfmsd125 = tf_msd(cumdata125).numpy() * 2\ntfmsd225 = tf_msd(cumdata225).numpy() * 2\nmae_tfmsd25 = mean_absolute_error(np.nan_to_num(tfmsd25, nan=1.0), Y25[0])\nmae_tfmsd65 = mean_absolute_error(np.nan_to_num(tfmsd65, nan=1.0), Y65[0])\nmae_tfmsd125 = mean_absolute_error(np.nan_to_num(tfmsd125, nan=1.0), Y125[0])\nmae_tfmsd225 = mean_absolute_error(np.nan_to_num(tfmsd225, nan=1.0), Y225[0])\n\n\nprint(\"TF_MSD MAE 25 :\", mae_tfmsd25)\nprint(\"TF_MSD MAE 65 :\", mae_tfmsd65)\nprint(\"TF_MSD MAE 125:\", mae_tfmsd125)\nprint(\"TF_MSD MAE 225:\", mae_tfmsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:59:27.936197Z","iopub.execute_input":"2025-06-01T17:59:27.936547Z","iopub.status.idle":"2025-06-01T18:06:55.593517Z","shell.execute_reply.started":"2025-06-01T17:59:27.936520Z","shell.execute_reply":"2025-06-01T18:06:55.592354Z"}},"outputs":[{"name":"stdout","text":"TF_MSD MAE 25 : 0.21519958044255197\nTF_MSD MAE 65 : 0.12887301655342648\nTF_MSD MAE 125: 0.09364387220227431\nTF_MSD MAE 225: 0.07115167039600138\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"pyhurstLayer = PyHurstEE()\npymsd25 = pyhurstLayer(cumdata25).detach().cpu().numpy() * 2\npymsd65 = pyhurstLayer(cumdata65).detach().cpu().numpy() * 2\npymsd125 = pyhurstLayer(cumdata125).detach().cpu().numpy() * 2\npymsd225 = pyhurstLayer(cumdata225).detach().cpu().numpy() * 2\nmae_pymsd25 = mean_absolute_error(np.nan_to_num(pymsd25, nan=1.0), Y25[0])\nmae_pymsd65 = mean_absolute_error(np.nan_to_num(pymsd65, nan=1.0), Y65[0])\nmae_pymsd125 = mean_absolute_error(np.nan_to_num(pymsd125, nan=1.0), Y125[0])\nmae_pymsd225 = mean_absolute_error(np.nan_to_num(pymsd225, nan=1.0), Y225[0])\n\n\nprint(\"PyHurstEELayer MAE 25 :\", mae_pymsd25)\nprint(\"PyHurstEELayer MAE 65 :\", mae_pymsd65)\nprint(\"PyHurstEELayer MAE 125:\", mae_pymsd125)\nprint(\"PyHurstEELayer MAE 225:\", mae_pymsd225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T18:06:55.594696Z","iopub.execute_input":"2025-06-01T18:06:55.595128Z","iopub.status.idle":"2025-06-01T18:06:58.941766Z","shell.execute_reply.started":"2025-06-01T18:06:55.595091Z","shell.execute_reply":"2025-06-01T18:06:58.940673Z"}},"outputs":[{"name":"stdout","text":"PyHurstEELayer MAE 25 : 0.2151995752717108\nPyHurstEELayer MAE 65 : 0.12887301154935754\nPyHurstEELayer MAE 125: 0.09364387645131714\nPyHurstEELayer MAE 225: 0.07115167474516139\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# Estimate Hurst exponent with statistical methods (FBM dataset)","metadata":{}},{"cell_type":"code","source":"H25 = [[] for _ in range(len(methods))]\nH65 = [[] for _ in range(len(methods))]\nH125 = [[] for _ in range(len(methods))]\nH225 = [[] for _ in range(len(methods))]\n\nminimal = 3\nmax_scale = 11\n\nfor row in range(len(X25[0])):\n    x25 = np.array(X25[0][row]).reshape(1,-1)\n    ts25=x25[:,1:]-x25[:,:-1]\n    ts25 = ts25 / np.std(ts25)\n    x65 = np.array(X65[0][row]).reshape(1,-1)\n    ts65=x65[:,1:]-x65[:,:-1]\n    ts65 = ts65 / np.std(ts65)\n    x125 = np.array(X125[0][row]).reshape(1,-1)\n    ts125 = x125[:,1:]-x125[:,:-1]\n    ts125 = ts125 / np.std(ts125)\n    x225 = np.array(X225[0][row]).reshape(1,-1)\n    ts225 = x225[:,1:]-x225[:,:-1]\n    ts225 = ts225 / np.std(ts225)\n    \n    H25[0].append(HSolver.EstHurstKS(ts25[0]) *2)\n    H25[1].append(HSolver.EstHurstGHE(ts25[0], q=1, method='L2') *2)\n    H25[2].append(HSolver.EstHurstGHE(ts25[0], q=3, method='L2') *2)\n    H25[3].append(HSolver.EstHurstHiguchi(ts25[0],7, method='L2') *2)\n    # H25[3].append(1)\n    H25[4].append(mf_dfa(ts25[0]) *2)\n    H25[5].append(whittle(ts25[0])  *2)\n    H25[6].append(HSolver.EstHurstTTA(ts25[0],7, method='L2')  *2)\n    H25[7].append(HSolver.EstHurstPeriodogram(ts25[0], cutoff=0.5, method='L2')  *2)\n    H25[8].append(HSolver.EstHurstRSAnalysis2(ts25[0],2, method='L2')  *2)\n    H25[9].append(calculate_Hw(ts25[0])  *2)\n    H25[10].append(HSolver.EstHurstLocalWhittle(ts25[0])  *2)\n    H25[11].append(HSolver.EstHurstLSSD(ts25[0], max_scale) *2)\n    H25[12].append(HSolver.EstHurstLSV(ts25[0], max_scale)  *2)\n    H25[13].append(HSolver.EstHurstGHE(ts25[0], method='L2')  *2)\n        \n    H65[0].append(HSolver.EstHurstKS(ts65[0])  *2)\n    H65[1].append(HSolver.EstHurstGHE(ts65[0], q=1, method='L2') *2)\n    H65[2].append(HSolver.EstHurstGHE(ts65[0],q=3, method='L2')  *2)\n    H65[3].append(HSolver.EstHurstHiguchi(ts65[0], method='L2')  *2)\n    H65[4].append(mf_dfa(ts65[0]) *2)\n    H65[5].append(whittle(ts65[0])  *2)\n    H65[6].append(HSolver.EstHurstTTA(ts65[0],7, method='L2')  *2)\n    H65[7].append(HSolver.EstHurstPeriodogram(ts65[0], cutoff=0.3, method='L2')  *2)\n    H65[8].append(HSolver.EstHurstRSAnalysis2(ts65[0],4, method='L2')  *2)\n    H65[9].append(calculate_Hw(ts65[0])  *2)\n    H65[10].append(HSolver.EstHurstLocalWhittle(ts65[0])  *2)\n    H65[11].append(HSolver.EstHurstLSSD(ts65[0], max_scale)  *2)\n    H65[12].append(HSolver.EstHurstLSV(ts65[0], max_scale)  *2)\n    H65[13].append(HSolver.EstHurstGHE(ts65[0], method='L2')  *2)\n\n\n    H125[0].append(HSolver.EstHurstKS(ts125[0])  *2)\n    H125[1].append(HSolver.EstHurstGHE(ts125[0], q=1, method='L2') *2)\n    H125[2].append(HSolver.EstHurstGHE(ts125[0],q=3, method='L2')  *2)\n    H125[3].append(HSolver.EstHurstHiguchi(ts125[0], method='L2')  *2)\n    H125[4].append(mf_dfa(ts125[0]) *2)\n    H125[5].append(whittle(ts125[0])  *2)\n    H125[6].append(HSolver.EstHurstTTA(ts125[0],7, method='L2')  *2)\n    H125[7].append(HSolver.EstHurstPeriodogram(ts125[0], cutoff=0.3, method='L2')  *2)\n    H125[8].append(HSolver.EstHurstRSAnalysis2(ts125[0],4, method='L2')  *2)\n    H125[9].append(calculate_Hw(ts125[0])  *2)\n    H125[10].append(HSolver.EstHurstLocalWhittle(ts125[0])  *2)\n    H125[11].append(HSolver.EstHurstLSSD(ts125[0], max_scale)  *2)\n    H125[12].append(HSolver.EstHurstLSV(ts125[0], max_scale)  *2)\n    H125[13].append(HSolver.EstHurstGHE(ts125[0], method='L2') *2)\n\n    H225[0].append(HSolver.EstHurstKS(ts225[0])  *2)\n    H225[1].append(HSolver.EstHurstGHE(ts225[0], q=1, method='L2') *2)\n    H225[2].append(HSolver.EstHurstGHE(ts225[0],q=3, method='L2')  *2)\n    H225[3].append(HSolver.EstHurstHiguchi(ts225[0], method='L2')  *2)\n    H225[4].append(mf_dfa(ts225[0]) *2)\n    H225[5].append(whittle(ts225[0])  *2)\n    H225[6].append(HSolver.EstHurstTTA(ts225[0],11, method='L2')  *2)\n    H225[7].append(HSolver.EstHurstPeriodogram(ts225[0], cutoff=0.3, method='L2')  *2)\n    H225[8].append(HSolver.EstHurstRSAnalysis2(ts225[0],4, method='L2')  *2)\n    H225[9].append(calculate_Hw(ts225[0])  *2)\n    H225[10].append(HSolver.EstHurstLocalWhittle(ts225[0])  *2)\n    H225[11].append(HSolver.EstHurstLSSD(ts225[0], max_scale)  *2)\n    H225[12].append(HSolver.EstHurstLSV(ts225[0], max_scale)  *2)\n    H225[13].append(HSolver.EstHurstGHE(ts225[0], method='L2')  *2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T18:06:58.942843Z","iopub.execute_input":"2025-06-01T18:06:58.943122Z","iopub.status.idle":"2025-06-01T18:41:06.394354Z","shell.execute_reply.started":"2025-06-01T18:06:58.943098Z","shell.execute_reply":"2025-06-01T18:41:06.392689Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"dic = {}\ndic['length'] = [25, 65, 125, 225]\nfor i in range(14):\n    dic[methods[i]] = [mean_absolute_error(np.nan_to_num(H25[i], nan=1.0), Y25[0]), mean_absolute_error(np.nan_to_num(H65[i], nan=1.0), Y65[0]),  mean_absolute_error(np.nan_to_num(H125[i], nan=1.0), Y125[0]), mean_absolute_error(np.nan_to_num(H225[i], nan=1.0), Y225[0])]\ndic[methods[14]] = [mae_tamsd25, mae_tamsd65, mae_tamsd125, mae_tamsd225]\ndic[methods[15]] = [mae_tfmsd25, mae_tfmsd65, mae_tfmsd125, mae_tfmsd225]\ndic[methods[16]] = [mae_pymsd25, mae_pymsd65, mae_pymsd125, mae_pymsd225]\ndic[methods[17]] = [mae_randi_25, mae_randi_65, mae_randi_125, mae_randi_225]\nfbm_df = pd.DataFrame(dic)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T18:41:06.396069Z","iopub.execute_input":"2025-06-01T18:41:06.396547Z","iopub.status.idle":"2025-06-01T18:41:06.552348Z","shell.execute_reply.started":"2025-06-01T18:41:06.396502Z","shell.execute_reply":"2025-06-01T18:41:06.551219Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Comparison of MAE for AnDi-2020 and FBM Datasets","metadata":{}},{"cell_type":"code","source":"print('AnDi-1:')\nandi1_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T18:41:06.553640Z","iopub.execute_input":"2025-06-01T18:41:06.554043Z","iopub.status.idle":"2025-06-01T18:41:06.578344Z","shell.execute_reply.started":"2025-06-01T18:41:06.554002Z","shell.execute_reply":"2025-06-01T18:41:06.577216Z"}},"outputs":[{"name":"stdout","text":"AnDi-1:\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n0      25  0.454171  0.443984  0.419161  0.452389  0.450180  0.453682   \n1      65  0.376811  0.396477  0.391624  0.384971  0.412330  0.415178   \n2     125  0.356891  0.371977  0.373319  0.363675  0.399313  0.395470   \n3     225  0.335924  0.362133  0.370919  0.335940  0.400207  0.384647   \n\n        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n0  0.732705  0.583806  0.843265  0.619631  0.511131  0.596974  0.527731   \n1  0.583644  0.411059  0.503674  0.484922  0.444520  0.485467  0.447116   \n2  0.523955  0.390735  0.442792  0.436275  0.414037  0.454038  0.419489   \n3  0.451076  0.376583  0.406967  0.413749  0.396574  0.436916  0.405759   \n\n     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n0  0.418598  0.418543    0.418687   0.418687  0.364368  \n1  0.387453  0.387420    0.387422   0.387422  0.283601  \n2  0.369599  0.369572    0.369719   0.369719  0.252379  \n3  0.365614  0.365591    0.365822   0.365822  0.256476  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>KS</th>\n      <th>GHE(1)</th>\n      <th>GHE(3)</th>\n      <th>HM</th>\n      <th>MFDFA(2)</th>\n      <th>Whittle</th>\n      <th>TTA</th>\n      <th>PM</th>\n      <th>RS</th>\n      <th>db4</th>\n      <th>LW</th>\n      <th>LSSD</th>\n      <th>LSV</th>\n      <th>GHE(2)</th>\n      <th>TA-MSD</th>\n      <th>TF HurstEE</th>\n      <th>PyHurstEE</th>\n      <th>RANDI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>0.454171</td>\n      <td>0.443984</td>\n      <td>0.419161</td>\n      <td>0.452389</td>\n      <td>0.450180</td>\n      <td>0.453682</td>\n      <td>0.732705</td>\n      <td>0.583806</td>\n      <td>0.843265</td>\n      <td>0.619631</td>\n      <td>0.511131</td>\n      <td>0.596974</td>\n      <td>0.527731</td>\n      <td>0.418598</td>\n      <td>0.418543</td>\n      <td>0.418687</td>\n      <td>0.418687</td>\n      <td>0.364368</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65</td>\n      <td>0.376811</td>\n      <td>0.396477</td>\n      <td>0.391624</td>\n      <td>0.384971</td>\n      <td>0.412330</td>\n      <td>0.415178</td>\n      <td>0.583644</td>\n      <td>0.411059</td>\n      <td>0.503674</td>\n      <td>0.484922</td>\n      <td>0.444520</td>\n      <td>0.485467</td>\n      <td>0.447116</td>\n      <td>0.387453</td>\n      <td>0.387420</td>\n      <td>0.387422</td>\n      <td>0.387422</td>\n      <td>0.283601</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>125</td>\n      <td>0.356891</td>\n      <td>0.371977</td>\n      <td>0.373319</td>\n      <td>0.363675</td>\n      <td>0.399313</td>\n      <td>0.395470</td>\n      <td>0.523955</td>\n      <td>0.390735</td>\n      <td>0.442792</td>\n      <td>0.436275</td>\n      <td>0.414037</td>\n      <td>0.454038</td>\n      <td>0.419489</td>\n      <td>0.369599</td>\n      <td>0.369572</td>\n      <td>0.369719</td>\n      <td>0.369719</td>\n      <td>0.252379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>225</td>\n      <td>0.335924</td>\n      <td>0.362133</td>\n      <td>0.370919</td>\n      <td>0.335940</td>\n      <td>0.400207</td>\n      <td>0.384647</td>\n      <td>0.451076</td>\n      <td>0.376583</td>\n      <td>0.406967</td>\n      <td>0.413749</td>\n      <td>0.396574</td>\n      <td>0.436916</td>\n      <td>0.405759</td>\n      <td>0.365614</td>\n      <td>0.365591</td>\n      <td>0.365822</td>\n      <td>0.365822</td>\n      <td>0.256476</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"print('FBM:')\nfbm_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T18:41:06.584449Z","iopub.execute_input":"2025-06-01T18:41:06.584806Z","iopub.status.idle":"2025-06-01T18:41:06.614472Z","shell.execute_reply.started":"2025-06-01T18:41:06.584764Z","shell.execute_reply":"2025-06-01T18:41:06.613132Z"}},"outputs":[{"name":"stdout","text":"FBM:\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"   length        KS    GHE(1)    GHE(3)        HM  MFDFA(2)   Whittle  \\\n0      25  0.313306  0.232267  0.219245  0.250572  0.302522  0.243385   \n1      65  0.154503  0.139394  0.133157  0.169242  0.232281  0.134028   \n2     125  0.111641  0.101935  0.096848  0.119221  0.214112  0.091115   \n3     225  0.085741  0.077383  0.073984  0.087798  0.207801  0.066406   \n\n        TTA        PM        RS       db4        LW      LSSD       LSV  \\\n0  0.529763  0.513029  0.720387  0.487608  0.297654  0.416648  0.324377   \n1  0.290682  0.236495  0.351217  0.292314  0.170139  0.224245  0.175540   \n2  0.197028  0.215608  0.278556  0.202311  0.127172  0.158282  0.126722   \n3  0.135921  0.213852  0.243371  0.201765  0.102942  0.114090  0.093446   \n\n     GHE(2)    TA-MSD  TF HurstEE  PyHurstEE     RANDI  \n0  0.215209  0.215161    0.215200   0.215200  0.312108  \n1  0.128876  0.128854    0.128873   0.128873  0.235522  \n2  0.093646  0.093631    0.093644   0.093644  0.209930  \n3  0.071153  0.071143    0.071152   0.071152  0.216112  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>KS</th>\n      <th>GHE(1)</th>\n      <th>GHE(3)</th>\n      <th>HM</th>\n      <th>MFDFA(2)</th>\n      <th>Whittle</th>\n      <th>TTA</th>\n      <th>PM</th>\n      <th>RS</th>\n      <th>db4</th>\n      <th>LW</th>\n      <th>LSSD</th>\n      <th>LSV</th>\n      <th>GHE(2)</th>\n      <th>TA-MSD</th>\n      <th>TF HurstEE</th>\n      <th>PyHurstEE</th>\n      <th>RANDI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>0.313306</td>\n      <td>0.232267</td>\n      <td>0.219245</td>\n      <td>0.250572</td>\n      <td>0.302522</td>\n      <td>0.243385</td>\n      <td>0.529763</td>\n      <td>0.513029</td>\n      <td>0.720387</td>\n      <td>0.487608</td>\n      <td>0.297654</td>\n      <td>0.416648</td>\n      <td>0.324377</td>\n      <td>0.215209</td>\n      <td>0.215161</td>\n      <td>0.215200</td>\n      <td>0.215200</td>\n      <td>0.312108</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65</td>\n      <td>0.154503</td>\n      <td>0.139394</td>\n      <td>0.133157</td>\n      <td>0.169242</td>\n      <td>0.232281</td>\n      <td>0.134028</td>\n      <td>0.290682</td>\n      <td>0.236495</td>\n      <td>0.351217</td>\n      <td>0.292314</td>\n      <td>0.170139</td>\n      <td>0.224245</td>\n      <td>0.175540</td>\n      <td>0.128876</td>\n      <td>0.128854</td>\n      <td>0.128873</td>\n      <td>0.128873</td>\n      <td>0.235522</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>125</td>\n      <td>0.111641</td>\n      <td>0.101935</td>\n      <td>0.096848</td>\n      <td>0.119221</td>\n      <td>0.214112</td>\n      <td>0.091115</td>\n      <td>0.197028</td>\n      <td>0.215608</td>\n      <td>0.278556</td>\n      <td>0.202311</td>\n      <td>0.127172</td>\n      <td>0.158282</td>\n      <td>0.126722</td>\n      <td>0.093646</td>\n      <td>0.093631</td>\n      <td>0.093644</td>\n      <td>0.093644</td>\n      <td>0.209930</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>225</td>\n      <td>0.085741</td>\n      <td>0.077383</td>\n      <td>0.073984</td>\n      <td>0.087798</td>\n      <td>0.207801</td>\n      <td>0.066406</td>\n      <td>0.135921</td>\n      <td>0.213852</td>\n      <td>0.243371</td>\n      <td>0.201765</td>\n      <td>0.102942</td>\n      <td>0.114090</td>\n      <td>0.093446</td>\n      <td>0.071153</td>\n      <td>0.071143</td>\n      <td>0.071152</td>\n      <td>0.071152</td>\n      <td>0.216112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}